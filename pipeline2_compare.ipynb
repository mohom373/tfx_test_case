{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tfx in /usr/local/lib/python3.7/site-packages (1.12.0)\n",
      "Requirement already satisfied: portpicker<2,>=1.3.1 in /usr/local/lib/python3.7/site-packages (from tfx) (1.5.2)\n",
      "Requirement already satisfied: google-apitools<1,>=0.5 in /usr/local/lib/python3.7/site-packages (from tfx) (0.5.31)\n",
      "Requirement already satisfied: ml-metadata<1.13.0,>=1.12.0 in /usr/local/lib/python3.7/site-packages (from tfx) (1.12.0)\n",
      "Requirement already satisfied: docker<5,>=4.1 in /usr/local/lib/python3.7/site-packages (from tfx) (4.4.4)\n",
      "Requirement already satisfied: protobuf<4,>=3.13 in /home/vscode/.local/lib/python3.7/site-packages (from tfx) (3.19.6)\n",
      "Requirement already satisfied: tfx-bsl<1.13.0,>=1.12.0 in /usr/local/lib/python3.7/site-packages (from tfx) (1.12.0)\n",
      "Requirement already satisfied: tensorflow-data-validation<1.13.0,>=1.12.0 in /usr/local/lib/python3.7/site-packages (from tfx) (1.12.0)\n",
      "Requirement already satisfied: tensorflow-model-analysis<0.44.0,>=0.43.0 in /usr/local/lib/python3.7/site-packages (from tfx) (0.43.0)\n",
      "Requirement already satisfied: attrs<22,>=19.3.0 in /usr/local/lib/python3.7/site-packages (from tfx) (21.4.0)\n",
      "Requirement already satisfied: numpy<2,>=1.16 in /usr/local/lib/python3.7/site-packages (from tfx) (1.21.6)\n",
      "Requirement already satisfied: typing-extensions<5,>=3.10.0.2 in /usr/local/lib/python3.7/site-packages (from tfx) (4.5.0)\n",
      "Requirement already satisfied: google-cloud-bigquery<3,>=2.26.0 in /usr/local/lib/python3.7/site-packages (from tfx) (2.34.4)\n",
      "Requirement already satisfied: pyyaml<6,>=3.12 in /usr/local/lib/python3.7/site-packages (from tfx) (5.4.1)\n",
      "Requirement already satisfied: ml-pipelines-sdk==1.12.0 in /usr/local/lib/python3.7/site-packages (from tfx) (1.12.0)\n",
      "Requirement already satisfied: tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15 in /usr/local/lib/python3.7/site-packages (from tfx) (2.11.0)\n",
      "Requirement already satisfied: tensorflow<2.12,>=2.11.0 in /usr/local/lib/python3.7/site-packages (from tfx) (2.11.0)\n",
      "Requirement already satisfied: google-api-core<1.33 in /usr/local/lib/python3.7/site-packages (from tfx) (1.32.0)\n",
      "Requirement already satisfied: google-cloud-aiplatform<1.18,>=1.6.2 in /usr/local/lib/python3.7/site-packages (from tfx) (1.17.1)\n",
      "Requirement already satisfied: jinja2<4,>=2.7.3 in /usr/local/lib/python3.7/site-packages (from tfx) (3.1.2)\n",
      "Requirement already satisfied: grpcio<2,>=1.28.1 in /usr/local/lib/python3.7/site-packages (from tfx) (1.53.0)\n",
      "Requirement already satisfied: kubernetes<13,>=10.0.1 in /usr/local/lib/python3.7/site-packages (from tfx) (12.0.1)\n",
      "Requirement already satisfied: tensorflow-transform<1.13.0,>=1.12.0 in /usr/local/lib/python3.7/site-packages (from tfx) (1.12.0)\n",
      "Requirement already satisfied: pyarrow<7,>=6 in /usr/local/lib/python3.7/site-packages (from tfx) (6.0.1)\n",
      "Requirement already satisfied: apache-beam[gcp]<3,>=2.40 in /usr/local/lib/python3.7/site-packages (from tfx) (2.46.0)\n",
      "Requirement already satisfied: click<8,>=7 in /usr/local/lib/python3.7/site-packages (from tfx) (7.1.2)\n",
      "Requirement already satisfied: absl-py<2.0.0,>=0.9 in /usr/local/lib/python3.7/site-packages (from tfx) (1.4.0)\n",
      "Requirement already satisfied: google-api-python-client<2,>=1.8 in /usr/local/lib/python3.7/site-packages (from tfx) (1.12.11)\n",
      "Requirement already satisfied: keras-tuner<2,>=1.0.4 in /usr/local/lib/python3.7/site-packages (from tfx) (1.3.4)\n",
      "Requirement already satisfied: tensorflow-hub<0.13,>=0.9.0 in /usr/local/lib/python3.7/site-packages (from tfx) (0.12.0)\n",
      "Requirement already satisfied: packaging<21,>=20 in /usr/local/lib/python3.7/site-packages (from tfx) (20.9)\n",
      "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.40->tfx) (3.13.0)\n",
      "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.40->tfx) (0.3.1.1)\n",
      "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.40->tfx) (1.7)\n",
      "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.40->tfx) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.40->tfx) (2.8.2)\n",
      "Requirement already satisfied: httplib2<0.22.0,>=0.8 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.40->tfx) (0.21.0)\n",
      "Requirement already satisfied: fasteners<1.0,>=0.3 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.40->tfx) (0.18)\n",
      "Requirement already satisfied: fastavro<2,>=0.23.6 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.40->tfx) (1.7.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.40->tfx) (2.28.2)\n",
      "Requirement already satisfied: orjson<4.0 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.40->tfx) (3.8.10)\n",
      "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.40->tfx) (2.7.0)\n",
      "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.40->tfx) (2023.3)\n",
      "Requirement already satisfied: regex>=2020.6.8 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.40->tfx) (2022.10.31)\n",
      "Requirement already satisfied: cloudpickle~=2.2.1 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.40->tfx) (2.2.1)\n",
      "Requirement already satisfied: proto-plus<2,>=1.7.1 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.40->tfx) (1.22.2)\n",
      "Requirement already satisfied: objsize<0.7.0,>=0.6.1 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.40->tfx) (0.6.1)\n",
      "Requirement already satisfied: zstandard<1,>=0.18.0 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.40->tfx) (0.20.0)\n",
      "Requirement already satisfied: google-auth-httplib2<0.2.0,>=0.1.0 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.40->tfx) (0.1.0)\n",
      "Requirement already satisfied: cachetools<5,>=3.1.0 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.40->tfx) (4.2.4)\n",
      "Requirement already satisfied: google-cloud-datastore<2,>=1.8.0 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.40->tfx) (1.15.5)\n",
      "Requirement already satisfied: google-cloud-spanner<4,>=3.0.0 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.40->tfx) (3.26.0)\n",
      "Requirement already satisfied: google-cloud-pubsub<3,>=2.1.0 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.40->tfx) (2.13.11)\n",
      "Requirement already satisfied: google-cloud-pubsublite<2,>=1.2.0 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.40->tfx) (1.6.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.18.0 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.40->tfx) (1.35.0)\n",
      "Requirement already satisfied: google-cloud-dlp<4,>=3.0.0 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.40->tfx) (3.9.2)\n",
      "Requirement already satisfied: google-cloud-bigquery-storage<2.17,>=2.6.3 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.40->tfx) (2.16.2)\n",
      "Requirement already satisfied: google-cloud-recommendations-ai<0.8.0,>=0.1.0 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.40->tfx) (0.7.1)\n",
      "Requirement already satisfied: google-cloud-bigtable<2,>=0.31.1 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.40->tfx) (1.7.3)\n",
      "Requirement already satisfied: google-cloud-vision<4,>=2 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.40->tfx) (3.1.4)\n",
      "Requirement already satisfied: google-cloud-videointelligence<2,>=1.8.0 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.40->tfx) (1.16.3)\n",
      "Requirement already satisfied: google-cloud-core<3,>=0.28.1 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.40->tfx) (2.3.2)\n",
      "Requirement already satisfied: google-cloud-language<2,>=1.3.0 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.40->tfx) (1.3.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.7/site-packages (from docker<5,>=4.1->tfx) (1.16.0)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.7/site-packages (from docker<5,>=4.1->tfx) (1.5.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/site-packages (from google-api-core<1.33->tfx) (1.59.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/site-packages (from google-api-core<1.33->tfx) (67.6.1)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/site-packages (from google-api-python-client<2,>=1.8->tfx) (3.0.1)\n",
      "Requirement already satisfied: oauth2client>=1.4.12 in /usr/local/lib/python3.7/site-packages (from google-apitools<1,>=0.5->tfx) (4.1.3)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.7/site-packages (from google-cloud-aiplatform<1.18,>=1.6.2->tfx) (2.8.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.7/site-packages (from google-cloud-aiplatform<1.18,>=1.6.2->tfx) (1.6.3)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.7/site-packages (from google-cloud-bigquery<3,>=2.26.0->tfx) (2.4.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.7/site-packages (from jinja2<4,>=2.7.3->tfx) (2.1.2)\n",
      "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.7/site-packages (from keras-tuner<2,>=1.0.4->tfx) (1.0.4)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.7/site-packages (from kubernetes<13,>=10.0.1->tfx) (1.26.15)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.7/site-packages (from kubernetes<13,>=10.0.1->tfx) (2022.12.7)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.7/site-packages (from kubernetes<13,>=10.0.1->tfx) (1.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/site-packages (from packaging<21,>=20->tfx) (3.0.9)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.7/site-packages (from portpicker<2,>=1.3.1->tfx) (5.9.4)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.7/site-packages (from tensorflow<2.12,>=2.11.0->tfx) (2.11.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/site-packages (from tensorflow<2.12,>=2.11.0->tfx) (2.2.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.7/site-packages (from tensorflow<2.12,>=2.11.0->tfx) (23.3.3)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/site-packages (from tensorflow<2.12,>=2.11.0->tfx) (3.8.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.7/site-packages (from tensorflow<2.12,>=2.11.0->tfx) (0.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/site-packages (from tensorflow<2.12,>=2.11.0->tfx) (1.15.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.7/site-packages (from tensorflow<2.12,>=2.11.0->tfx) (2.11.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/site-packages (from tensorflow<2.12,>=2.11.0->tfx) (0.2.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/site-packages (from tensorflow<2.12,>=2.11.0->tfx) (0.32.0)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.7/site-packages (from tensorflow<2.12,>=2.11.0->tfx) (2.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/site-packages (from tensorflow<2.12,>=2.11.0->tfx) (16.0.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/site-packages (from tensorflow<2.12,>=2.11.0->tfx) (3.3.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/site-packages (from tensorflow<2.12,>=2.11.0->tfx) (1.6.3)\n",
      "Requirement already satisfied: tensorflow-metadata<1.13,>=1.12.0 in /usr/local/lib/python3.7/site-packages (from tensorflow-data-validation<1.13.0,>=1.12.0->tfx) (1.12.0)\n",
      "Requirement already satisfied: pandas<2,>=1.0 in /usr/local/lib/python3.7/site-packages (from tensorflow-data-validation<1.13.0,>=1.12.0->tfx) (1.3.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.7/site-packages (from tensorflow-data-validation<1.13.0,>=1.12.0->tfx) (1.2.0)\n",
      "Requirement already satisfied: pyfarmhash<0.4,>=0.2 in /usr/local/lib/python3.7/site-packages (from tensorflow-data-validation<1.13.0,>=1.12.0->tfx) (0.3.2)\n",
      "Requirement already satisfied: ipython<8,>=7 in /usr/local/lib/python3.7/site-packages (from tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (7.34.0)\n",
      "Requirement already satisfied: ipywidgets<8,>=7 in /home/vscode/.local/lib/python3.7/site-packages (from tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (7.7.5)\n",
      "Requirement already satisfied: scipy<2,>=1.4.1 in /usr/local/lib/python3.7/site-packages (from tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (1.7.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow<2.12,>=2.11.0->tfx) (0.40.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/site-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.40->tfx) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/site-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.40->tfx) (4.9)\n",
      "Requirement already satisfied: grpc-google-iam-v1<0.13dev,>=0.12.3 in /usr/local/lib/python3.7/site-packages (from google-cloud-bigtable<2,>=0.31.1->apache-beam[gcp]<3,>=2.40->tfx) (0.12.6)\n",
      "Requirement already satisfied: grpcio-status>=1.16.0 in /usr/local/lib/python3.7/site-packages (from google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]<3,>=2.40->tfx) (1.48.2)\n",
      "Requirement already satisfied: overrides<7.0.0,>=6.0.1 in /usr/local/lib/python3.7/site-packages (from google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp]<3,>=2.40->tfx) (6.5.0)\n",
      "Requirement already satisfied: sqlparse>=0.3.0 in /usr/local/lib/python3.7/site-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]<3,>=2.40->tfx) (0.4.3)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3,>=2.26.0->tfx) (1.5.0)\n",
      "Requirement already satisfied: docopt in /usr/local/lib/python3.7/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.40->tfx) (0.6.2)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.7/site-packages (from ipython<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (2.14.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/site-packages (from ipython<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (4.8.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.7/site-packages (from ipython<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (3.0.38)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/site-packages (from ipython<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (5.9.0)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.7/site-packages (from ipython<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/site-packages (from ipython<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (0.1.6)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/site-packages (from ipython<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (0.18.2)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.7/site-packages (from ipython<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (5.1.1)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/site-packages (from ipython<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (0.7.5)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/site-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (0.2.0)\n",
      "Requirement already satisfied: jupyterlab-widgets<3,>=1.0.0 in /home/vscode/.local/lib/python3.7/site-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (1.1.4)\n",
      "Requirement already satisfied: widgetsnbextension~=3.6.4 in /home/vscode/.local/lib/python3.7/site-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (3.6.4)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/site-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (6.16.2)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/site-packages (from oauth2client>=1.4.12->google-apitools<1,>=0.5->tfx) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.7/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.40->tfx) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.40->tfx) (3.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tfx) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tfx) (3.4.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tfx) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tfx) (2.2.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tfx) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<13,>=10.0.1->tfx) (3.2.2)\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (1.5.6)\n",
      "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (25.0.2)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (7.4.9)\n",
      "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (1.6.7)\n",
      "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (6.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/site-packages (from jedi>=0.16->ipython<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (0.8.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tfx) (6.3.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/site-packages (from pexpect>4.3->ipython<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (0.2.6)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/site-packages (from widgetsnbextension~=3.6.4->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (6.5.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tfx) (3.15.0)\n",
      "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (0.4)\n",
      "Requirement already satisfied: jupyter-core>=4.9.2 in /usr/local/lib/python3.7/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (4.12.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (0.17.1)\n",
      "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (7.3.0)\n",
      "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (21.3.0)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (1.8.0)\n",
      "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (0.5.5)\n",
      "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (0.16.0)\n",
      "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (5.8.0)\n",
      "Requirement already satisfied: notebook-shim>=0.1.0 in /usr/local/lib/python3.7/site-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (0.2.2)\n",
      "Requirement already satisfied: jupyter-server>=1.8 in /usr/local/lib/python3.7/site-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (1.23.6)\n",
      "Requirement already satisfied: mistune<3,>=2.0.3 in /usr/local/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (2.0.5)\n",
      "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (1.2.1)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (0.7.1)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (0.7.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (4.12.2)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (6.0.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (1.5.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (0.2.2)\n",
      "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/site-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (2.16.3)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/site-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (4.17.3)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.7/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (21.2.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (5.12.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (0.19.3)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in /usr/local/lib/python3.7/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (1.3.10)\n",
      "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.7/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (3.6.2)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.7/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (1.15.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.7/site-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (2.4)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/site-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (0.5.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.7/site-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (1.3.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.4->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (2.21)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U tfx\n",
    "#!pip install --user tfx tensorflow Pillow tensorflow_datasets matplotlib azure-storage-blob object-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.11.0\n",
      "TFX version: 1.12.0\n"
     ]
    }
   ],
   "source": [
    "# Import tensorflow and TFX modules\n",
    "import tensorflow as tf\n",
    "print('TensorFlow version: {}'.format(tf.__version__))\n",
    "from tfx import v1 as tfx\n",
    "print('TFX version: {}'.format(tfx.__version__))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PIPELINE_NAME = \"penguin-pipeline2\"\n",
    "\n",
    "# Output directory to store artifacts generated from the pipeline.\n",
    "PIPELINE_ROOT = os.path.join('pipelines', PIPELINE_NAME)\n",
    "# Path to a SQLite DB file to use as an MLMD storage.\n",
    "METADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db')\n",
    "# Output directory where created models from the pipeline will be exported.\n",
    "SERVING_MODEL_DIR = os.path.join('serving_model', PIPELINE_NAME)\n",
    "\n",
    "from absl import logging\n",
    "logging.set_verbosity(logging.INFO)  # Set default logging level.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREP DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('penguin-data/processed/data-processed.csv',\n",
       " <http.client.HTTPMessage at 0x7fece76e4390>)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "import tempfile\n",
    "\n",
    "#processed data file\n",
    "\n",
    "DATA_ROOT = os.path.join('penguin-data')\n",
    "PROCESSED_DATA_ROOT = os.path.join(DATA_ROOT, 'processed')\n",
    "\n",
    "os.makedirs(DATA_ROOT + '/processed', exist_ok=True)\n",
    "\n",
    "_data_path = 'https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/penguin/data/labelled/penguins_processed.csv'\n",
    "_data_filepath = os.path.join(DATA_ROOT,'processed', 'data-processed.csv')\n",
    "urllib.request.urlretrieve(_data_path, _data_filepath)\n",
    "\n",
    "#Non processed data\n",
    "\n",
    "#DATA_ROOT = os.path.join('penguin-data')\n",
    "#FULL_DATA_ROOT = os.path.join(DATA_ROOT, 'full-set')\n",
    "#INCOMPLETE_DATA_ROOT = os.path.join(DATA_ROOT, 'incomplete-set')\n",
    "\n",
    "#os.makedirs(DATA_ROOT + '/full-set', exist_ok=True)\n",
    "#_data_path = 'https://storage.googleapis.com/download.tensorflow.org/data/palmer_penguins/penguins_size.csv'\n",
    "#_data_filepath = os.path.join(DATA_ROOT,'full-set', 'data-full.csv')\n",
    "#urllib.request.urlretrieve(_data_path, _data_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "species,culmen_length_mm,culmen_depth_mm,flipper_length_mm,body_mass_g\n",
      "0,0.2545454545454545,0.6666666666666666,0.15254237288135594,0.2916666666666667\n",
      "0,0.26909090909090905,0.5119047619047618,0.23728813559322035,0.3055555555555556\n",
      "0,0.29818181818181805,0.5833333333333334,0.3898305084745763,0.1527777777777778\n",
      "0,0.16727272727272732,0.7380952380952381,0.3559322033898305,0.20833333333333334\n",
      "0,0.26181818181818167,0.892857142857143,0.3050847457627119,0.2638888888888889\n",
      "0,0.24727272727272717,0.5595238095238096,0.15254237288135594,0.2569444444444444\n",
      "0,0.25818181818181823,0.773809523809524,0.3898305084745763,0.5486111111111112\n",
      "0,0.32727272727272727,0.5357142857142859,0.1694915254237288,0.1388888888888889\n",
      "0,0.23636363636363636,0.9642857142857142,0.3220338983050847,0.3055555555555556\n"
     ]
    }
   ],
   "source": [
    "!head {_data_filepath}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DELETE ENTRIES WITH NA FIELDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!sed -i '/\\bNA\\b/d' {_data_filepath}\n",
    "#!head {_data_filepath}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create multiple datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove 50 entries of Chinstrap species\n",
    "#%run remove_csv.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE DATA.CSV file with incomplete dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATING SCHEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('schema/schema.pbtxt', <http.client.HTTPMessage at 0x7fece76dad90>)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "SCHEMA_PATH = 'schema'\n",
    "\n",
    "_schema_uri = 'https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/penguin/schema/raw/schema.pbtxt'\n",
    "_schema_filename = 'schema.pbtxt'\n",
    "_schema_filepath = os.path.join(SCHEMA_PATH, _schema_filename)\n",
    "\n",
    "os.makedirs(SCHEMA_PATH, exist_ok=True)\n",
    "urllib.request.urlretrieve(_schema_uri, _schema_filepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE FILES FOR COMPONENT FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "_module_file = 'penguin_utils.py'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting penguin_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {_module_file}\n",
    "\n",
    "\n",
    "from typing import List, Text\n",
    "from absl import logging\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow_metadata.proto.v0 import schema_pb2\n",
    "import tensorflow_transform as tft\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "\n",
    "from tfx import v1 as tfx\n",
    "from tfx_bsl.public import tfxio\n",
    "\n",
    "# Specify features that we will use.\n",
    "_FEATURE_KEYS = [\n",
    "    'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'\n",
    "]\n",
    "_LABEL_KEY = 'species'\n",
    "\n",
    "_TRAIN_BATCH_SIZE = 20\n",
    "_EVAL_BATCH_SIZE = 10\n",
    "\n",
    "\n",
    "# NEW: TFX Transform will call this function.\n",
    "def preprocessing_fn(inputs):\n",
    "  \"\"\"tf.transform's callback function for preprocessing inputs.\n",
    "\n",
    "  Args:\n",
    "    inputs: map from feature keys to raw not-yet-transformed features.\n",
    "\n",
    "  Returns:\n",
    "    Map from string feature key to transformed feature.\n",
    "  \"\"\"\n",
    "  outputs = {}\n",
    "\n",
    "  # Uses features defined in _FEATURE_KEYS only.\n",
    "  for key in _FEATURE_KEYS:\n",
    "    # tft.scale_to_z_score computes the mean and variance of the given feature\n",
    "    # and scales the output based on the result.\n",
    "    outputs[key] = tft.scale_to_z_score(inputs[key])\n",
    "\n",
    "  # For the label column we provide the mapping from string to index.\n",
    "  # We could instead use `tft.compute_and_apply_vocabulary()` in order to\n",
    "  # compute the vocabulary dynamically and perform a lookup.\n",
    "  # Since in this example there are only 3 possible values, we use a hard-coded\n",
    "  # table for simplicity.\n",
    "  table_keys = ['Adelie', 'Chinstrap', 'Gentoo']\n",
    "  initializer = tf.lookup.KeyValueTensorInitializer(\n",
    "      keys=table_keys,\n",
    "      values=tf.cast(tf.range(len(table_keys)), tf.int64),\n",
    "      key_dtype=tf.string,\n",
    "      value_dtype=tf.int64)\n",
    "  table = tf.lookup.StaticHashTable(initializer, default_value=-1)\n",
    "  outputs[_LABEL_KEY] = table.lookup(inputs[_LABEL_KEY])\n",
    "\n",
    "  return outputs\n",
    "\n",
    "\n",
    "# NEW: This function will apply the same transform operation to training data\n",
    "#      and serving requests.\n",
    "def _apply_preprocessing(raw_features, tft_layer):\n",
    "  transformed_features = tft_layer(raw_features)\n",
    "  if _LABEL_KEY in raw_features:\n",
    "    transformed_label = transformed_features.pop(_LABEL_KEY)\n",
    "    return transformed_features, transformed_label\n",
    "  else:\n",
    "    return transformed_features, None\n",
    "\n",
    "\n",
    "# NEW: This function will create a handler function which gets a serialized\n",
    "#      tf.example, preprocess and run an inference with it.\n",
    "def _get_serve_tf_examples_fn(model, tf_transform_output):\n",
    "  # We must save the tft_layer to the model to ensure its assets are kept and\n",
    "  # tracked.\n",
    "  model.tft_layer = tf_transform_output.transform_features_layer()\n",
    "\n",
    "  @tf.function(input_signature=[\n",
    "      tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')\n",
    "  ])\n",
    "  def serve_tf_examples_fn(serialized_tf_examples):\n",
    "    # Expected input is a string which is serialized tf.Example format.\n",
    "    feature_spec = tf_transform_output.raw_feature_spec()\n",
    "    # Because input schema includes unnecessary fields like 'species' and\n",
    "    # 'island', we filter feature_spec to include required keys only.\n",
    "    required_feature_spec = {\n",
    "        k: v for k, v in feature_spec.items() if k in _FEATURE_KEYS\n",
    "    }\n",
    "    parsed_features = tf.io.parse_example(serialized_tf_examples,\n",
    "                                          required_feature_spec)\n",
    "\n",
    "    # Preprocess parsed input with transform operation defined in\n",
    "    # preprocessing_fn().\n",
    "    transformed_features, _ = _apply_preprocessing(parsed_features,\n",
    "                                                   model.tft_layer)\n",
    "    # Run inference with ML model.\n",
    "    return model(transformed_features)\n",
    "\n",
    "  return serve_tf_examples_fn\n",
    "\n",
    "\n",
    "def _input_fn(file_pattern: List[Text],\n",
    "              data_accessor: tfx.components.DataAccessor,\n",
    "              tf_transform_output: tft.TFTransformOutput,\n",
    "              batch_size: int = 200) -> tf.data.Dataset:\n",
    "  \"\"\"Generates features and label for tuning/training.\n",
    "\n",
    "  Args:\n",
    "    file_pattern: List of paths or patterns of input tfrecord files.\n",
    "    data_accessor: DataAccessor for converting input to RecordBatch.\n",
    "    tf_transform_output: A TFTransformOutput.\n",
    "    batch_size: representing the number of consecutive elements of returned\n",
    "      dataset to combine in a single batch\n",
    "\n",
    "  Returns:\n",
    "    A dataset that contains (features, indices) tuple where features is a\n",
    "      dictionary of Tensors, and indices is a single Tensor of label indices.\n",
    "  \"\"\"\n",
    "  dataset = data_accessor.tf_dataset_factory(\n",
    "      file_pattern,\n",
    "      tfxio.TensorFlowDatasetOptions(batch_size=batch_size),\n",
    "      schema=tf_transform_output.raw_metadata.schema)\n",
    "\n",
    "  transform_layer = tf_transform_output.transform_features_layer()\n",
    "  def apply_transform(raw_features):\n",
    "    return _apply_preprocessing(raw_features, transform_layer)\n",
    "\n",
    "  return dataset.map(apply_transform).repeat()\n",
    "\n",
    "\n",
    "def _build_keras_model() -> tf.keras.Model:\n",
    "  \"\"\"Creates a DNN Keras model for classifying penguin data.\n",
    "\n",
    "  Returns:\n",
    "    A Keras Model.\n",
    "  \"\"\"\n",
    "  # The model below is built with Functional API, please refer to\n",
    "  # https://www.tensorflow.org/guide/keras/overview for all API options.\n",
    "  inputs = [\n",
    "      keras.layers.Input(shape=(1,), name=key)\n",
    "      for key in _FEATURE_KEYS\n",
    "  ]\n",
    "  d = keras.layers.concatenate(inputs)\n",
    "  for _ in range(2):\n",
    "    d = keras.layers.Dense(8, activation='relu')(d)\n",
    "  outputs = keras.layers.Dense(3)(d)\n",
    "\n",
    "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "  model.compile(\n",
    "      optimizer=keras.optimizers.Adam(1e-2),\n",
    "      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "      metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "  model.summary(print_fn=logging.info)\n",
    "  return model\n",
    "\n",
    "\n",
    "# TFX Trainer will call this function.\n",
    "def run_fn(fn_args: tfx.components.FnArgs):\n",
    "  \"\"\"Train the model based on given args.\n",
    "\n",
    "  Args:\n",
    "    fn_args: Holds args used to train the model as name/value pairs.\n",
    "  \"\"\"\n",
    "  tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n",
    "\n",
    "  train_dataset = _input_fn(\n",
    "      fn_args.train_files,\n",
    "      fn_args.data_accessor,\n",
    "      tf_transform_output,\n",
    "      batch_size=_TRAIN_BATCH_SIZE)\n",
    "  eval_dataset = _input_fn(\n",
    "      fn_args.eval_files,\n",
    "      fn_args.data_accessor,\n",
    "      tf_transform_output,\n",
    "      batch_size=_EVAL_BATCH_SIZE)\n",
    "\n",
    "  model = _build_keras_model()\n",
    "  model.fit(\n",
    "      train_dataset,\n",
    "      steps_per_epoch=fn_args.train_steps,\n",
    "      validation_data=eval_dataset,\n",
    "      validation_steps=fn_args.eval_steps)\n",
    "\n",
    "  # NEW: Save a computation graph including transform layer.\n",
    "  signatures = {\n",
    "      'serving_default': _get_serve_tf_examples_fn(model, tf_transform_output),\n",
    "  }\n",
    "  model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting penguin_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {_module_file}\n",
    "\n",
    "# Copied from https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple\n",
    "\n",
    "from typing import List\n",
    "from absl import logging\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "\n",
    "from tfx.components.trainer.executor import TrainerFnArgs\n",
    "from tfx.components.trainer.fn_args_utils import DataAccessor\n",
    "from tfx_bsl.tfxio import dataset_options\n",
    "from tensorflow_metadata.proto.v0 import schema_pb2\n",
    "\n",
    "_FEATURE_KEYS = [\n",
    "    'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'\n",
    "]\n",
    "_LABEL_KEY = 'species'\n",
    "\n",
    "_TRAIN_BATCH_SIZE = 20\n",
    "_EVAL_BATCH_SIZE = 10\n",
    "\n",
    "# Since we're not generating or creating a schema, we will instead create\n",
    "# a feature spec.  Since there are a fairly small number of features this is\n",
    "# manageable for this dataset.\n",
    "_FEATURE_SPEC = {\n",
    "    **{\n",
    "        feature: tf.io.FixedLenFeature(shape=[1], dtype=tf.float32)\n",
    "           for feature in _FEATURE_KEYS\n",
    "       },\n",
    "    _LABEL_KEY: tf.io.FixedLenFeature(shape=[1], dtype=tf.int64)\n",
    "}\n",
    "\n",
    "\n",
    "def _input_fn(file_pattern: List[str],\n",
    "              data_accessor: DataAccessor,\n",
    "              schema: schema_pb2.Schema,\n",
    "              batch_size: int = 200) -> tf.data.Dataset:\n",
    "  \"\"\"Generates features and label for training.\n",
    "\n",
    "  Args:\n",
    "    file_pattern: List of paths or patterns of input tfrecord files.\n",
    "    data_accessor: DataAccessor for converting input to RecordBatch.\n",
    "    schema: schema of the input data.\n",
    "    batch_size: representing the number of consecutive elements of returned\n",
    "      dataset to combine in a single batch\n",
    "\n",
    "  Returns:\n",
    "    A dataset that contains (features, indices) tuple where features is a\n",
    "      dictionary of Tensors, and indices is a single Tensor of label indices.\n",
    "  \"\"\"\n",
    "  return data_accessor.tf_dataset_factory(\n",
    "      file_pattern,\n",
    "      dataset_options.TensorFlowDatasetOptions(\n",
    "          batch_size=batch_size, label_key=_LABEL_KEY),\n",
    "      schema=schema).repeat()\n",
    "\n",
    "\n",
    "def _build_keras_model() -> tf.keras.Model:\n",
    "  \"\"\"Creates a DNN Keras model for classifying penguin data.\n",
    "\n",
    "  Returns:\n",
    "    A Keras Model.\n",
    "  \"\"\"\n",
    "  # The model below is built with Functional API, please refer to\n",
    "  # https://www.tensorflow.org/guide/keras/overview for all API options.\n",
    "  inputs = [keras.layers.Input(shape=(1,), name=f) for f in _FEATURE_KEYS]\n",
    "  d = keras.layers.concatenate(inputs)\n",
    "  for _ in range(2):\n",
    "    d = keras.layers.Dense(8, activation='relu')(d)\n",
    "  outputs = keras.layers.Dense(3)(d)\n",
    "\n",
    "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "  model.compile(\n",
    "      optimizer=keras.optimizers.Adam(1e-2),\n",
    "      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "      metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "  model.summary(print_fn=logging.info)\n",
    "  return model\n",
    "\n",
    "\n",
    "# TFX Trainer will call this function.\n",
    "def run_fn(fn_args: TrainerFnArgs):\n",
    "  \"\"\"Train the model based on given args.\n",
    "\n",
    "  Args:\n",
    "    fn_args: Holds args used to train the model as name/value pairs.\n",
    "  \"\"\"\n",
    "\n",
    "  # This schema is usually either an output of SchemaGen or a manually-curated\n",
    "  # version provided by pipeline author. A schema can also derived from TFT\n",
    "  # graph if a Transform component is used. In the case when either is missing,\n",
    "  # `schema_from_feature_spec` could be used to generate schema from very simple\n",
    "  # feature_spec, but the schema returned would be very primitive.\n",
    "  schema = schema_utils.schema_from_feature_spec(_FEATURE_SPEC)\n",
    "  \n",
    "  train_dataset = _input_fn(\n",
    "      fn_args.train_files,\n",
    "      fn_args.data_accessor,\n",
    "      schema,\n",
    "      batch_size=_TRAIN_BATCH_SIZE)\n",
    "  eval_dataset = _input_fn(\n",
    "      fn_args.eval_files,\n",
    "      fn_args.data_accessor,\n",
    "      schema,\n",
    "      batch_size=_EVAL_BATCH_SIZE)\n",
    "\n",
    "  model = _build_keras_model()\n",
    "  model.fit(\n",
    "      train_dataset,\n",
    "      steps_per_epoch=fn_args.train_steps,\n",
    "      validation_data=eval_dataset,\n",
    "      validation_steps=fn_args.eval_steps)\n",
    "\n",
    "  # The result of the training should be saved in `fn_args.serving_model_dir`\n",
    "  # directory.\n",
    "  model.save(fn_args.serving_model_dir, save_format='tf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIPELINE DEFINITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel(\n",
      "    type_name: Model\n",
      "    artifacts: []\n",
      "    additional_properties: {'hej': 'svej'}\n",
      "    additional_custom_properties: {}\n",
      ")\n",
      "EXTERNAL CHANNEL:  ExternalProjectChannel(project_owner=penguin_pipeline1, project_name=penguin_pipeline1, mlmd_service_target=metadata/penguin-pipeline1, pipeline_name=penguin-pipeline1, producer_component_id=Trainer, output_key=model, pipeline_run_id=2023-04-27T09:11:05.045733)\n"
     ]
    }
   ],
   "source": [
    "import ml_metadata as mlmd\n",
    "from ml_metadata.metadata_store import metadata_store\n",
    "from ml_metadata.proto import metadata_store_pb2\n",
    "from tfx.types.channel_utils import external_project_artifact_query\n",
    "import pprint\n",
    "\n",
    "\"\"\"\n",
    "def create_custom_model_artifact(model_path):\n",
    "    model_artifact = tfx.types.standard_artifacts.Model()\n",
    "    model_artifact.uri = model_path\n",
    "    model_artifact.pipeline_name = 'penguin-pipeline1'\n",
    "    model_artifact.set_int_custom_property('artifact_id', 999999) # Use a unique artifact ID\n",
    "    return model_artifact\n",
    "\"\"\"\n",
    "# Define a custom filter function to filter out the custom model artifact\n",
    "#def filter_baseline_model(artifact: metadata_store_pb2.Artifact):\n",
    "#    return artifact.mlmd_artifact.custom_properties['artifact_id'].int_value != 999999\n",
    "\n",
    "#model_path = 'pipelines/penguin-pipeline1/Trainer/model/5'\n",
    "#print(create_custom_model_artifact(model_path))\n",
    "#print(filter_baseline_model(create_custom_model_artifact('pipelines/penguin-pipeline1/Trainer/model/2')))\n",
    " \n",
    "# Create a custom Model artifact pointing to the saved_model.pb file\n",
    "#custom_model_artifact = create_custom_model_artifact(model_path)\n",
    "\n",
    "# Create a Model channel pointing to the custom Model artifact\n",
    "baseline_model_channel = tfx.dsl.Channel(type=tfx.types.standard_artifacts.Model)\n",
    "baseline_model_channel.artifacts = [\"Hej svej go\", \"brotha\"]\n",
    "baseline_model_channel.additional_properties = {'hej': 'svej'}\n",
    "print(baseline_model_channel)\n",
    "\n",
    "external_pipeline_channel = external_project_artifact_query(\n",
    "    artifact_type=tfx.types.standard_artifacts.Model,\n",
    "    pipeline_name='penguin-pipeline1',\n",
    "    pipeline_run_id='2023-04-27T09:11:05.045733',\n",
    "    producer_component_id='Trainer', \n",
    "    output_key='model',\n",
    "    project_name=\"penguin_pipeline1\",\n",
    "    project_owner=\"penguin_pipeline1\",\n",
    "    mlmd_service_target=\"metadata/penguin-pipeline1\"\n",
    "    )\n",
    "\n",
    "print('EXTERNAL CHANNEL: ', external_pipeline_channel)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_analysis as tfma\n",
    "\n",
    "def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,\n",
    "                      module_file: str, serving_model_dir: str,\n",
    "                     metadata_path: str) -> tfx.dsl.Pipeline:\n",
    "    \"\"\"Implements the penguin pipeline with TFX.\"\"\"\n",
    "    # Brings data into the pipeline or otherwise joins/converts training data.\n",
    "    example_gen = tfx.components.CsvExampleGen(input_base=data_root)\n",
    "\n",
    "    # Uses user-provided Python function that trains a model.\n",
    "    trainer = tfx.components.Trainer(\n",
    "        module_file=module_file,\n",
    "        examples=example_gen.outputs['examples'],\n",
    "\n",
    "        # NEW: Pass transform_graph to the trainer.\n",
    "        #transform_graph=transform.outputs['transform_graph'],\n",
    "\n",
    "        train_args=tfx.proto.TrainArgs(num_steps=100),\n",
    "        eval_args=tfx.proto.EvalArgs(num_steps=5))\n",
    "\n",
    "\n",
    "# we want evaluator functionality here\n",
    "  # NEW: Uses TFMA to compute evaluation statistics over features of a model and\n",
    "  #   perform quality validation of a candidate model (compared to a baseline).\n",
    "\n",
    "    eval_config = tfma.EvalConfig(\n",
    "      model_specs=[tfma.ModelSpec(label_key='species')],\n",
    "      slicing_specs=[\n",
    "          # An empty slice spec means the overall slice, i.e. the whole dataset.\n",
    "          tfma.SlicingSpec(),\n",
    "          # Calculate metrics for each penguin species.\n",
    "          tfma.SlicingSpec(feature_keys=['species']),\n",
    "          ],\n",
    "      metrics_specs=[\n",
    "          tfma.MetricsSpec(per_slice_thresholds={\n",
    "              'sparse_categorical_accuracy':\n",
    "                  tfma.PerSliceMetricThresholds(thresholds=[\n",
    "                      tfma.PerSliceMetricThreshold(\n",
    "                          slicing_specs=[tfma.SlicingSpec()],\n",
    "                          threshold=tfma.MetricThreshold(\n",
    "                              value_threshold=tfma.GenericValueThreshold(\n",
    "                                   lower_bound={'value': 0.6}),\n",
    "                              # Change threshold will be ignored if there is no\n",
    "                              # baseline model resolved from MLMD (first run).\n",
    "                              change_threshold=tfma.GenericChangeThreshold(\n",
    "                                  direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n",
    "                                  absolute={'value': -1e-10}))\n",
    "                       )]),\n",
    "          })],\n",
    "      )\n",
    "\n",
    "\n",
    "        \n",
    "    # NEW: Get the latest blessed model for Evaluator.\n",
    "    model_resolver = tfx.dsl.Resolver(\n",
    "        strategy_class=tfx.dsl.experimental.LatestBlessedModelStrategy,\n",
    "        model=tfx.dsl.Channel(type=tfx.types.standard_artifacts.Model),\n",
    "        model_blessing=tfx.dsl.Channel(\n",
    "            type=tfx.types.standard_artifacts.ModelBlessing)).with_id(\n",
    "        'latest_blessed_model_resolver')\n",
    "    \n",
    "    evaluator = tfx.components.Evaluator(\n",
    "        examples=example_gen.outputs['examples'],\n",
    "        model=trainer.outputs['model'],\n",
    "        baseline_model=external_pipeline_channel,\n",
    "        #baseline_model=model_resolver.outputs['model'],\n",
    "        # Change threshold will be ignored if there is no baseline (first run).\n",
    "        eval_config=eval_config)\n",
    "\n",
    "    # Pushes the model to a filesystem destination.\n",
    "    pusher = tfx.components.Pusher(\n",
    "        model=trainer.outputs['model'],\n",
    "        model_blessing=evaluator.outputs['blessing'],\n",
    "        push_destination=tfx.proto.PushDestination(\n",
    "            filesystem=tfx.proto.PushDestination.Filesystem(\n",
    "                base_directory=serving_model_dir)))\n",
    "\n",
    "    components = [\n",
    "        example_gen,\n",
    "        #statistics_gen,\n",
    "        #schema_importer,\n",
    "        #example_validator,\n",
    "\n",
    "        #transform,  # NEW: Transform component was added to the pipeline.\n",
    "\n",
    "        trainer,\n",
    "        model_resolver,\n",
    "        evaluator,  #evaluator\n",
    "        pusher,\n",
    "    ]\n",
    "\n",
    "    return tfx.dsl.Pipeline(\n",
    "        pipeline_name=pipeline_name,\n",
    "        pipeline_root=pipeline_root,\n",
    "        metadata_connection_config=tfx.orchestration.metadata\n",
    "        .sqlite_metadata_connection_config(metadata_path),\n",
    "        components=components)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Generating ephemeral wheel package for '/workspaces/tfx_test_case/penguin_utils.py' (including modules: ['penguin_utils', 'remove_csv']).\n",
      "INFO:absl:User module package has hash fingerprint version 2e96f662907f75f30f530f04b3aab5018be8a1ca9ae45f8f8bc0c97ab53843d0.\n",
      "INFO:absl:Executing: ['/usr/local/bin/python', '/tmp/tmptmahof1o/_tfx_generated_setup.py', 'bdist_wheel', '--bdist-dir', '/tmp/tmplduxcr7r', '--dist-dir', '/tmp/tmpohm6ospn']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying penguin_utils.py -> build/lib\n",
      "copying remove_csv.py -> build/lib\n",
      "installing to /tmp/tmplduxcr7r\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/penguin_utils.py -> /tmp/tmplduxcr7r\n",
      "copying build/lib/remove_csv.py -> /tmp/tmplduxcr7r\n",
      "running install_egg_info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  setuptools.SetuptoolsDeprecationWarning,\n",
      "INFO:absl:Successfully built user code wheel distribution at 'pipelines/penguin-pipeline2/_wheels/tfx_user_code_Trainer-0.0+2e96f662907f75f30f530f04b3aab5018be8a1ca9ae45f8f8bc0c97ab53843d0-py3-none-any.whl'; target user module is 'penguin_utils'.\n",
      "INFO:absl:Full user module path is 'penguin_utils@pipelines/penguin-pipeline2/_wheels/tfx_user_code_Trainer-0.0+2e96f662907f75f30f530f04b3aab5018be8a1ca9ae45f8f8bc0c97ab53843d0-py3-none-any.whl'\n",
      "INFO:absl:Using deployment config:\n",
      " executor_specs {\n",
      "  key: \"CsvExampleGen\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.components.example_gen.csv_example_gen.executor.Executor\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"Evaluator\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.components.evaluator.executor.Executor\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"Pusher\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.pusher.executor.Executor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"Trainer\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.trainer.executor.GenericExecutor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "custom_driver_specs {\n",
      "  key: \"CsvExampleGen\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.example_gen.driver.FileBasedDriver\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "metadata_connection_config {\n",
      "  database_connection_config {\n",
      "    sqlite {\n",
      "      filename_uri: \"metadata/penguin-pipeline2/metadata.db\"\n",
      "      connection_mode: READWRITE_OPENCREATE\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:Using connection config:\n",
      " sqlite {\n",
      "  filename_uri: \"metadata/penguin-pipeline2/metadata.db\"\n",
      "  connection_mode: READWRITE_OPENCREATE\n",
      "}\n",
      "\n",
      "INFO:absl:Component CsvExampleGen is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.example_gen.csv_example_gen.component.CsvExampleGen\"\n",
      "  }\n",
      "  id: \"CsvExampleGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-pipeline2\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2023-04-27T11:58:59.502336\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-pipeline2.CsvExampleGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"input_base\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"penguin-data/processed\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"input_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"*\\\"\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 2,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_data_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 6\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_file_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 5\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"Evaluator\"\n",
      "downstream_nodes: \"Trainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:[CsvExampleGen] Resolved inputs: ({},)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running egg_info\n",
      "creating tfx_user_code_Trainer.egg-info\n",
      "writing tfx_user_code_Trainer.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_Trainer.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_Trainer.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_Trainer.egg-info to /tmp/tmplduxcr7r/tfx_user_code_Trainer-0.0+2e96f662907f75f30f530f04b3aab5018be8a1ca9ae45f8f8bc0c97ab53843d0-py3.7.egg-info\n",
      "running install_scripts\n",
      "creating /tmp/tmplduxcr7r/tfx_user_code_Trainer-0.0+2e96f662907f75f30f530f04b3aab5018be8a1ca9ae45f8f8bc0c97ab53843d0.dist-info/WHEEL\n",
      "creating '/tmp/tmpohm6ospn/tfx_user_code_Trainer-0.0+2e96f662907f75f30f530f04b3aab5018be8a1ca9ae45f8f8bc0c97ab53843d0-py3-none-any.whl' and adding '/tmp/tmplduxcr7r' to it\n",
      "adding 'penguin_utils.py'\n",
      "adding 'remove_csv.py'\n",
      "adding 'tfx_user_code_Trainer-0.0+2e96f662907f75f30f530f04b3aab5018be8a1ca9ae45f8f8bc0c97ab53843d0.dist-info/METADATA'\n",
      "adding 'tfx_user_code_Trainer-0.0+2e96f662907f75f30f530f04b3aab5018be8a1ca9ae45f8f8bc0c97ab53843d0.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_Trainer-0.0+2e96f662907f75f30f530f04b3aab5018be8a1ca9ae45f8f8bc0c97ab53843d0.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_Trainer-0.0+2e96f662907f75f30f530f04b3aab5018be8a1ca9ae45f8f8bc0c97ab53843d0.dist-info/RECORD'\n",
      "removing /tmp/tmplduxcr7r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:select span and version = (0, None)\n",
      "INFO:absl:latest span and version = (0, None)\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Going to run a new execution 43\n",
      "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=43, input_dict={}, output_dict=defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"pipelines/penguin-pipeline2/CsvExampleGen/examples/43\"\n",
      "custom_properties {\n",
      "  key: \"input_fingerprint\"\n",
      "  value {\n",
      "    string_value: \"split:single_split,num_files:1,total_bytes:25648,xor_checksum:1682589380,sum_checksum:1682589380\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}), exec_properties={'input_config': '{\\n  \"splits\": [\\n    {\\n      \"name\": \"single_split\",\\n      \"pattern\": \"*\"\\n    }\\n  ]\\n}', 'input_base': 'penguin-data/processed', 'output_file_format': 5, 'output_data_format': 6, 'output_config': '{\\n  \"split_config\": {\\n    \"splits\": [\\n      {\\n        \"hash_buckets\": 2,\\n        \"name\": \"train\"\\n      },\\n      {\\n        \"hash_buckets\": 1,\\n        \"name\": \"eval\"\\n      }\\n    ]\\n  }\\n}', 'span': 0, 'version': None, 'input_fingerprint': 'split:single_split,num_files:1,total_bytes:25648,xor_checksum:1682589380,sum_checksum:1682589380'}, execution_output_uri='pipelines/penguin-pipeline2/CsvExampleGen/.system/executor_execution/43/executor_output.pb', stateful_working_dir='pipelines/penguin-pipeline2/CsvExampleGen/.system/stateful_working_dir/2023-04-27T11:58:59.502336', tmp_dir='pipelines/penguin-pipeline2/CsvExampleGen/.system/executor_execution/43/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.example_gen.csv_example_gen.component.CsvExampleGen\"\n",
      "  }\n",
      "  id: \"CsvExampleGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-pipeline2\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2023-04-27T11:58:59.502336\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-pipeline2.CsvExampleGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"input_base\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"penguin-data/processed\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"input_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"*\\\"\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 2,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_data_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 6\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_file_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 5\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"Evaluator\"\n",
      "downstream_nodes: \"Trainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"penguin-pipeline2\"\n",
      ", pipeline_run_id='2023-04-27T11:58:59.502336')\n",
      "INFO:absl:Generating examples.\n",
      "INFO:absl:Processing input csv data penguin-data/processed/* to TFExample.\n",
      "INFO:absl:Examples generated.\n",
      "INFO:absl:Value type <class 'NoneType'> of key version in exec_properties is not supported, going to drop it\n",
      "INFO:absl:Value type <class 'list'> of key _beam_pipeline_args in exec_properties is not supported, going to drop it\n",
      "INFO:absl:Cleaning up stateless execution info.\n",
      "INFO:absl:Execution 43 succeeded.\n",
      "INFO:absl:Cleaning up stateful execution info.\n",
      "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"pipelines/penguin-pipeline2/CsvExampleGen/examples/43\"\n",
      "custom_properties {\n",
      "  key: \"input_fingerprint\"\n",
      "  value {\n",
      "    string_value: \"split:single_split,num_files:1,total_bytes:25648,xor_checksum:1682589380,sum_checksum:1682589380\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}) for execution 43\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Component CsvExampleGen is finished.\n",
      "INFO:absl:Component latest_blessed_model_resolver is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.dsl.components.common.resolver.Resolver\"\n",
      "  }\n",
      "  id: \"latest_blessed_model_resolver\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-pipeline2\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2023-04-27T11:58:59.502336\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-pipeline2.latest_blessed_model_resolver\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"_generated_model_3\"\n",
      "    value {\n",
      "      channels {\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-pipeline2\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "            base_type: MODEL\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      hidden: true\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"_generated_modelblessing_4\"\n",
      "    value {\n",
      "      channels {\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-pipeline2\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"ModelBlessing\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      hidden: true\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      input_graph_ref {\n",
      "        graph_id: \"graph_1\"\n",
      "        key: \"model\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"model_blessing\"\n",
      "    value {\n",
      "      input_graph_ref {\n",
      "        graph_id: \"graph_1\"\n",
      "        key: \"model_blessing\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  input_graphs {\n",
      "    key: \"graph_1\"\n",
      "    value {\n",
      "      nodes {\n",
      "        key: \"dict_2\"\n",
      "        value {\n",
      "          output_data_type: ARTIFACT_MULTIMAP\n",
      "          dict_node {\n",
      "            node_ids {\n",
      "              key: \"model\"\n",
      "              value: \"input_3\"\n",
      "            }\n",
      "            node_ids {\n",
      "              key: \"model_blessing\"\n",
      "              value: \"input_4\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      nodes {\n",
      "        key: \"input_3\"\n",
      "        value {\n",
      "          output_data_type: ARTIFACT_LIST\n",
      "          input_node {\n",
      "            input_key: \"_generated_model_3\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      nodes {\n",
      "        key: \"input_4\"\n",
      "        value {\n",
      "          output_data_type: ARTIFACT_LIST\n",
      "          input_node {\n",
      "            input_key: \"_generated_modelblessing_4\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      nodes {\n",
      "        key: \"op_1\"\n",
      "        value {\n",
      "          output_data_type: ARTIFACT_MULTIMAP\n",
      "          op_node {\n",
      "            op_type: \"tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy.LatestBlessedModelStrategy\"\n",
      "            args {\n",
      "              node_id: \"dict_2\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      result_node: \"op_1\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:Running as an resolver node.\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "INFO:absl:[latest_blessed_model_resolver] Resolved inputs: ({'model': [Artifact(artifact: id: 51\n",
      "type_id: 17\n",
      "uri: \"pipelines/penguin-pipeline2/Trainer/model/40\"\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.12.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1682590236537\n",
      "last_update_time_since_epoch: 1682590236537\n",
      ", artifact_type: id: 17\n",
      "name: \"Model\"\n",
      "base_type: MODEL\n",
      ")], 'model_blessing': [Artifact(artifact: id: 52\n",
      "type_id: 20\n",
      "uri: \"pipelines/penguin-pipeline2/Evaluator/blessing/41\"\n",
      "custom_properties {\n",
      "  key: \"blessed\"\n",
      "  value {\n",
      "    int_value: 1\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"current_model\"\n",
      "  value {\n",
      "    string_value: \"pipelines/penguin-pipeline2/Trainer/model/40\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"current_model_id\"\n",
      "  value {\n",
      "    int_value: 51\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.12.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1682590241454\n",
      "last_update_time_since_epoch: 1682590241454\n",
      ", artifact_type: id: 20\n",
      "name: \"ModelBlessing\"\n",
      ")]},)\n",
      "INFO:absl:Component latest_blessed_model_resolver is finished.\n",
      "INFO:absl:Component Trainer is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.trainer.component.Trainer\"\n",
      "    base_type: TRAIN\n",
      "  }\n",
      "  id: \"Trainer\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-pipeline2\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2023-04-27T11:58:59.502336\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-pipeline2.Trainer\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"CsvExampleGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-pipeline2\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2023-04-27T11:58:59.502336\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-pipeline2.CsvExampleGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Model\"\n",
      "          base_type: MODEL\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"model_run\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ModelRun\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"eval_args\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"num_steps\\\": 5\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"module_path\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"penguin_utils@pipelines/penguin-pipeline2/_wheels/tfx_user_code_Trainer-0.0+2e96f662907f75f30f530f04b3aab5018be8a1ca9ae45f8f8bc0c97ab53843d0-py3-none-any.whl\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"train_args\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"num_steps\\\": 100\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"CsvExampleGen\"\n",
      "downstream_nodes: \"Evaluator\"\n",
      "downstream_nodes: \"Pusher\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "INFO:absl:[Trainer] Resolved inputs: ({'examples': [Artifact(artifact: id: 55\n",
      "type_id: 15\n",
      "uri: \"pipelines/penguin-pipeline2/CsvExampleGen/examples/43\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"file_format\"\n",
      "  value {\n",
      "    string_value: \"tfrecords_gzip\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"input_fingerprint\"\n",
      "  value {\n",
      "    string_value: \"split:single_split,num_files:1,total_bytes:25648,xor_checksum:1682589380,sum_checksum:1682589380\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"payload_format\"\n",
      "  value {\n",
      "    string_value: \"FORMAT_TF_EXAMPLE\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.12.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1682596741922\n",
      "last_update_time_since_epoch: 1682596741922\n",
      ", artifact_type: id: 15\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]},)\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Going to run a new execution 45\n",
      "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=45, input_dict={'examples': [Artifact(artifact: id: 55\n",
      "type_id: 15\n",
      "uri: \"pipelines/penguin-pipeline2/CsvExampleGen/examples/43\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"file_format\"\n",
      "  value {\n",
      "    string_value: \"tfrecords_gzip\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"input_fingerprint\"\n",
      "  value {\n",
      "    string_value: \"split:single_split,num_files:1,total_bytes:25648,xor_checksum:1682589380,sum_checksum:1682589380\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"payload_format\"\n",
      "  value {\n",
      "    string_value: \"FORMAT_TF_EXAMPLE\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.12.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1682596741922\n",
      "last_update_time_since_epoch: 1682596741922\n",
      ", artifact_type: id: 15\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}, output_dict=defaultdict(<class 'list'>, {'model_run': [Artifact(artifact: uri: \"pipelines/penguin-pipeline2/Trainer/model_run/45\"\n",
      ", artifact_type: name: \"ModelRun\"\n",
      ")], 'model': [Artifact(artifact: uri: \"pipelines/penguin-pipeline2/Trainer/model/45\"\n",
      ", artifact_type: name: \"Model\"\n",
      "base_type: MODEL\n",
      ")]}), exec_properties={'eval_args': '{\\n  \"num_steps\": 5\\n}', 'custom_config': 'null', 'module_path': 'penguin_utils@pipelines/penguin-pipeline2/_wheels/tfx_user_code_Trainer-0.0+2e96f662907f75f30f530f04b3aab5018be8a1ca9ae45f8f8bc0c97ab53843d0-py3-none-any.whl', 'train_args': '{\\n  \"num_steps\": 100\\n}'}, execution_output_uri='pipelines/penguin-pipeline2/Trainer/.system/executor_execution/45/executor_output.pb', stateful_working_dir='pipelines/penguin-pipeline2/Trainer/.system/stateful_working_dir/2023-04-27T11:58:59.502336', tmp_dir='pipelines/penguin-pipeline2/Trainer/.system/executor_execution/45/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.trainer.component.Trainer\"\n",
      "    base_type: TRAIN\n",
      "  }\n",
      "  id: \"Trainer\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-pipeline2\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2023-04-27T11:58:59.502336\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-pipeline2.Trainer\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"CsvExampleGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-pipeline2\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2023-04-27T11:58:59.502336\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-pipeline2.CsvExampleGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Model\"\n",
      "          base_type: MODEL\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"model_run\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ModelRun\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"eval_args\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"num_steps\\\": 5\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"module_path\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"penguin_utils@pipelines/penguin-pipeline2/_wheels/tfx_user_code_Trainer-0.0+2e96f662907f75f30f530f04b3aab5018be8a1ca9ae45f8f8bc0c97ab53843d0-py3-none-any.whl\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"train_args\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"num_steps\\\": 100\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"CsvExampleGen\"\n",
      "downstream_nodes: \"Evaluator\"\n",
      "downstream_nodes: \"Pusher\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"penguin-pipeline2\"\n",
      ", pipeline_run_id='2023-04-27T11:58:59.502336')\n",
      "INFO:absl:Train on the 'train' split when train_args.splits is not set.\n",
      "INFO:absl:Evaluate on the 'eval' split when eval_args.splits is not set.\n",
      "INFO:absl:udf_utils.get_fn {'eval_args': '{\\n  \"num_steps\": 5\\n}', 'custom_config': 'null', 'module_path': 'penguin_utils@pipelines/penguin-pipeline2/_wheels/tfx_user_code_Trainer-0.0+2e96f662907f75f30f530f04b3aab5018be8a1ca9ae45f8f8bc0c97ab53843d0-py3-none-any.whl', 'train_args': '{\\n  \"num_steps\": 100\\n}'} 'run_fn'\n",
      "INFO:absl:Installing 'pipelines/penguin-pipeline2/_wheels/tfx_user_code_Trainer-0.0+2e96f662907f75f30f530f04b3aab5018be8a1ca9ae45f8f8bc0c97ab53843d0-py3-none-any.whl' to a temporary directory.\n",
      "INFO:absl:Executing: ['/usr/local/bin/python', '-m', 'pip', 'install', '--target', '/tmp/tmp73421h_0', 'pipelines/penguin-pipeline2/_wheels/tfx_user_code_Trainer-0.0+2e96f662907f75f30f530f04b3aab5018be8a1ca9ae45f8f8bc0c97ab53843d0-py3-none-any.whl']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./pipelines/penguin-pipeline2/_wheels/tfx_user_code_Trainer-0.0+2e96f662907f75f30f530f04b3aab5018be8a1ca9ae45f8f8bc0c97ab53843d0-py3-none-any.whl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.1.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n",
      "INFO:absl:Successfully installed 'pipelines/penguin-pipeline2/_wheels/tfx_user_code_Trainer-0.0+2e96f662907f75f30f530f04b3aab5018be8a1ca9ae45f8f8bc0c97ab53843d0-py3-none-any.whl'.\n",
      "INFO:absl:Training model.\n",
      "INFO:absl:Feature body_mass_g has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature culmen_depth_mm has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature culmen_length_mm has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature flipper_length_mm has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature species has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: tfx-user-code-Trainer\n",
      "Successfully installed tfx-user-code-Trainer-0.0+2e96f662907f75f30f530f04b3aab5018be8a1ca9ae45f8f8bc0c97ab53843d0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Feature body_mass_g has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature culmen_depth_mm has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature culmen_length_mm has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature flipper_length_mm has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature species has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature body_mass_g has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature culmen_depth_mm has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature culmen_length_mm has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature flipper_length_mm has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature species has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature body_mass_g has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature culmen_depth_mm has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature culmen_length_mm has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature flipper_length_mm has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature species has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Model: \"model_19\"\n",
      "INFO:absl:__________________________________________________________________________________________________\n",
      "INFO:absl: Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "INFO:absl:==================================================================================================\n",
      "INFO:absl: culmen_length_mm (InputLayer)  [(None, 1)]          0           []                               \n",
      "INFO:absl:                                                                                                  \n",
      "INFO:absl: culmen_depth_mm (InputLayer)   [(None, 1)]          0           []                               \n",
      "INFO:absl:                                                                                                  \n",
      "INFO:absl: flipper_length_mm (InputLayer)  [(None, 1)]         0           []                               \n",
      "INFO:absl:                                                                                                  \n",
      "INFO:absl: body_mass_g (InputLayer)       [(None, 1)]          0           []                               \n",
      "INFO:absl:                                                                                                  \n",
      "INFO:absl: concatenate_19 (Concatenate)   (None, 4)            0           ['culmen_length_mm[0][0]',       \n",
      "INFO:absl:                                                                  'culmen_depth_mm[0][0]',        \n",
      "INFO:absl:                                                                  'flipper_length_mm[0][0]',      \n",
      "INFO:absl:                                                                  'body_mass_g[0][0]']            \n",
      "INFO:absl:                                                                                                  \n",
      "INFO:absl: dense_57 (Dense)               (None, 8)            40          ['concatenate_19[0][0]']         \n",
      "INFO:absl:                                                                                                  \n",
      "INFO:absl: dense_58 (Dense)               (None, 8)            72          ['dense_57[0][0]']               \n",
      "INFO:absl:                                                                                                  \n",
      "INFO:absl: dense_59 (Dense)               (None, 3)            27          ['dense_58[0][0]']               \n",
      "INFO:absl:                                                                                                  \n",
      "INFO:absl:==================================================================================================\n",
      "INFO:absl:Total params: 139\n",
      "INFO:absl:Trainable params: 139\n",
      "INFO:absl:Non-trainable params: 0\n",
      "INFO:absl:__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 2s 7ms/step - loss: 0.7368 - sparse_categorical_accuracy: 0.6785 - val_loss: 0.4744 - val_sparse_categorical_accuracy: 0.8200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: pipelines/penguin-pipeline2/Trainer/model/45/Format-Serving/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: pipelines/penguin-pipeline2/Trainer/model/45/Format-Serving/assets\n",
      "INFO:absl:Training complete. Model written to pipelines/penguin-pipeline2/Trainer/model/45/Format-Serving. ModelRun written to pipelines/penguin-pipeline2/Trainer/model_run/45\n",
      "INFO:absl:Cleaning up stateless execution info.\n",
      "INFO:absl:Execution 45 succeeded.\n",
      "INFO:absl:Cleaning up stateful execution info.\n",
      "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'model_run': [Artifact(artifact: uri: \"pipelines/penguin-pipeline2/Trainer/model_run/45\"\n",
      ", artifact_type: name: \"ModelRun\"\n",
      ")], 'model': [Artifact(artifact: uri: \"pipelines/penguin-pipeline2/Trainer/model/45\"\n",
      ", artifact_type: name: \"Model\"\n",
      "base_type: MODEL\n",
      ")]}) for execution 45\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Component Trainer is finished.\n",
      "INFO:absl:Component Evaluator is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.evaluator.component.Evaluator\"\n",
      "    base_type: EVALUATE\n",
      "  }\n",
      "  id: \"Evaluator\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-pipeline2\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2023-04-27T11:58:59.502336\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-pipeline2.Evaluator\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"baseline_model\"\n",
      "    value {\n",
      "      channels {\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguinipeline9\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguinipeline9.Trainer\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2023-04-27T09:11:05.045733\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "            base_type: MODEL\n",
      "          }\n",
      "        }\n",
      "        output_key: \"model\"\n",
      "        metadata_connection_config {\n",
      "          type_url: \"type.googleapis.com/tfx.orchestration.MLMDServiceConfig\"\n",
      "          value: \"\\n\\021penguin_pipeline9\\022\\021penguin_pipeline9\\032\\031metadata/penguinpipeline9\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"CsvExampleGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-pipeline2\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2023-04-27T11:58:59.502336\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-pipeline2.CsvExampleGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Trainer\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-pipeline2\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2023-04-27T11:58:59.502336\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-pipeline2.Trainer\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "            base_type: MODEL\n",
      "          }\n",
      "        }\n",
      "        output_key: \"model\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"blessing\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ModelBlessing\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"evaluation\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ModelEvaluation\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"eval_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"metrics_specs\\\": [\\n    {\\n      \\\"per_slice_thresholds\\\": {\\n        \\\"sparse_categorical_accuracy\\\": {\\n          \\\"thresholds\\\": [\\n            {\\n              \\\"slicing_specs\\\": [\\n                {}\\n              ],\\n              \\\"threshold\\\": {\\n                \\\"change_threshold\\\": {\\n                  \\\"absolute\\\": -1e-10,\\n                  \\\"direction\\\": \\\"HIGHER_IS_BETTER\\\"\\n                },\\n                \\\"value_threshold\\\": {\\n                  \\\"lower_bound\\\": 0.6\\n                }\\n              }\\n            }\\n          ]\\n        }\\n      }\\n    }\\n  ],\\n  \\\"model_specs\\\": [\\n    {\\n      \\\"label_key\\\": \\\"species\\\"\\n    }\\n  ],\\n  \\\"slicing_specs\\\": [\\n    {},\\n    {\\n      \\\"feature_keys\\\": [\\n        \\\"species\\\"\\n      ]\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"example_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"fairness_indicator_thresholds\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"CsvExampleGen\"\n",
      "upstream_nodes: \"Trainer\"\n",
      "downstream_nodes: \"Pusher\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "INFO:absl:[Evaluator] Resolved inputs: ({'baseline_model': [], 'model': [Artifact(artifact: id: 57\n",
      "type_id: 17\n",
      "uri: \"pipelines/penguin-pipeline2/Trainer/model/45\"\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.12.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1682596750775\n",
      "last_update_time_since_epoch: 1682596750775\n",
      ", artifact_type: id: 17\n",
      "name: \"Model\"\n",
      "base_type: MODEL\n",
      ")], 'examples': [Artifact(artifact: id: 55\n",
      "type_id: 15\n",
      "uri: \"pipelines/penguin-pipeline2/CsvExampleGen/examples/43\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"file_format\"\n",
      "  value {\n",
      "    string_value: \"tfrecords_gzip\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"input_fingerprint\"\n",
      "  value {\n",
      "    string_value: \"split:single_split,num_files:1,total_bytes:25648,xor_checksum:1682589380,sum_checksum:1682589380\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"payload_format\"\n",
      "  value {\n",
      "    string_value: \"FORMAT_TF_EXAMPLE\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.12.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1682596741922\n",
      "last_update_time_since_epoch: 1682596741922\n",
      ", artifact_type: id: 15\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]},)\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Going to run a new execution 46\n",
      "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=46, input_dict={'baseline_model': [], 'model': [Artifact(artifact: id: 57\n",
      "type_id: 17\n",
      "uri: \"pipelines/penguin-pipeline2/Trainer/model/45\"\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.12.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1682596750775\n",
      "last_update_time_since_epoch: 1682596750775\n",
      ", artifact_type: id: 17\n",
      "name: \"Model\"\n",
      "base_type: MODEL\n",
      ")], 'examples': [Artifact(artifact: id: 55\n",
      "type_id: 15\n",
      "uri: \"pipelines/penguin-pipeline2/CsvExampleGen/examples/43\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"file_format\"\n",
      "  value {\n",
      "    string_value: \"tfrecords_gzip\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"input_fingerprint\"\n",
      "  value {\n",
      "    string_value: \"split:single_split,num_files:1,total_bytes:25648,xor_checksum:1682589380,sum_checksum:1682589380\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"payload_format\"\n",
      "  value {\n",
      "    string_value: \"FORMAT_TF_EXAMPLE\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.12.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1682596741922\n",
      "last_update_time_since_epoch: 1682596741922\n",
      ", artifact_type: id: 15\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}, output_dict=defaultdict(<class 'list'>, {'evaluation': [Artifact(artifact: uri: \"pipelines/penguin-pipeline2/Evaluator/evaluation/46\"\n",
      ", artifact_type: name: \"ModelEvaluation\"\n",
      ")], 'blessing': [Artifact(artifact: uri: \"pipelines/penguin-pipeline2/Evaluator/blessing/46\"\n",
      ", artifact_type: name: \"ModelBlessing\"\n",
      ")]}), exec_properties={'fairness_indicator_thresholds': 'null', 'example_splits': 'null', 'eval_config': '{\\n  \"metrics_specs\": [\\n    {\\n      \"per_slice_thresholds\": {\\n        \"sparse_categorical_accuracy\": {\\n          \"thresholds\": [\\n            {\\n              \"slicing_specs\": [\\n                {}\\n              ],\\n              \"threshold\": {\\n                \"change_threshold\": {\\n                  \"absolute\": -1e-10,\\n                  \"direction\": \"HIGHER_IS_BETTER\"\\n                },\\n                \"value_threshold\": {\\n                  \"lower_bound\": 0.6\\n                }\\n              }\\n            }\\n          ]\\n        }\\n      }\\n    }\\n  ],\\n  \"model_specs\": [\\n    {\\n      \"label_key\": \"species\"\\n    }\\n  ],\\n  \"slicing_specs\": [\\n    {},\\n    {\\n      \"feature_keys\": [\\n        \"species\"\\n      ]\\n    }\\n  ]\\n}'}, execution_output_uri='pipelines/penguin-pipeline2/Evaluator/.system/executor_execution/46/executor_output.pb', stateful_working_dir='pipelines/penguin-pipeline2/Evaluator/.system/stateful_working_dir/2023-04-27T11:58:59.502336', tmp_dir='pipelines/penguin-pipeline2/Evaluator/.system/executor_execution/46/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.evaluator.component.Evaluator\"\n",
      "    base_type: EVALUATE\n",
      "  }\n",
      "  id: \"Evaluator\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-pipeline2\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2023-04-27T11:58:59.502336\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-pipeline2.Evaluator\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"baseline_model\"\n",
      "    value {\n",
      "      channels {\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguinipeline9\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguinipeline9.Trainer\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2023-04-27T09:11:05.045733\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "            base_type: MODEL\n",
      "          }\n",
      "        }\n",
      "        output_key: \"model\"\n",
      "        metadata_connection_config {\n",
      "          type_url: \"type.googleapis.com/tfx.orchestration.MLMDServiceConfig\"\n",
      "          value: \"\\n\\021penguin_pipeline9\\022\\021penguin_pipeline9\\032\\031metadata/penguinpipeline9\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"CsvExampleGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-pipeline2\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2023-04-27T11:58:59.502336\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-pipeline2.CsvExampleGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Trainer\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-pipeline2\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2023-04-27T11:58:59.502336\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-pipeline2.Trainer\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "            base_type: MODEL\n",
      "          }\n",
      "        }\n",
      "        output_key: \"model\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"blessing\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ModelBlessing\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"evaluation\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ModelEvaluation\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"eval_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"metrics_specs\\\": [\\n    {\\n      \\\"per_slice_thresholds\\\": {\\n        \\\"sparse_categorical_accuracy\\\": {\\n          \\\"thresholds\\\": [\\n            {\\n              \\\"slicing_specs\\\": [\\n                {}\\n              ],\\n              \\\"threshold\\\": {\\n                \\\"change_threshold\\\": {\\n                  \\\"absolute\\\": -1e-10,\\n                  \\\"direction\\\": \\\"HIGHER_IS_BETTER\\\"\\n                },\\n                \\\"value_threshold\\\": {\\n                  \\\"lower_bound\\\": 0.6\\n                }\\n              }\\n            }\\n          ]\\n        }\\n      }\\n    }\\n  ],\\n  \\\"model_specs\\\": [\\n    {\\n      \\\"label_key\\\": \\\"species\\\"\\n    }\\n  ],\\n  \\\"slicing_specs\\\": [\\n    {},\\n    {\\n      \\\"feature_keys\\\": [\\n        \\\"species\\\"\\n      ]\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"example_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"fairness_indicator_thresholds\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"CsvExampleGen\"\n",
      "upstream_nodes: \"Trainer\"\n",
      "downstream_nodes: \"Pusher\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"penguin-pipeline2\"\n",
      ", pipeline_run_id='2023-04-27T11:58:59.502336')\n",
      "INFO:absl:udf_utils.get_fn {'fairness_indicator_thresholds': 'null', 'example_splits': 'null', 'eval_config': '{\\n  \"metrics_specs\": [\\n    {\\n      \"per_slice_thresholds\": {\\n        \"sparse_categorical_accuracy\": {\\n          \"thresholds\": [\\n            {\\n              \"slicing_specs\": [\\n                {}\\n              ],\\n              \"threshold\": {\\n                \"change_threshold\": {\\n                  \"absolute\": -1e-10,\\n                  \"direction\": \"HIGHER_IS_BETTER\"\\n                },\\n                \"value_threshold\": {\\n                  \"lower_bound\": 0.6\\n                }\\n              }\\n            }\\n          ]\\n        }\\n      }\\n    }\\n  ],\\n  \"model_specs\": [\\n    {\\n      \"label_key\": \"species\"\\n    }\\n  ],\\n  \"slicing_specs\": [\\n    {},\\n    {\\n      \"feature_keys\": [\\n        \"species\"\\n      ]\\n    }\\n  ]\\n}'} 'custom_eval_shared_model'\n",
      "INFO:absl:Request was made to ignore the baseline ModelSpec and any change thresholds. This is likely because a baseline model was not provided: updated_config=\n",
      "model_specs {\n",
      "  label_key: \"species\"\n",
      "}\n",
      "slicing_specs {\n",
      "}\n",
      "slicing_specs {\n",
      "  feature_keys: \"species\"\n",
      "}\n",
      "metrics_specs {\n",
      "  per_slice_thresholds {\n",
      "    key: \"sparse_categorical_accuracy\"\n",
      "    value {\n",
      "      thresholds {\n",
      "        slicing_specs {\n",
      "        }\n",
      "        threshold {\n",
      "          value_threshold {\n",
      "            lower_bound {\n",
      "              value: 0.6\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:Using pipelines/penguin-pipeline2/Trainer/model/45/Format-Serving as  model.\n",
      "INFO:absl:The 'example_splits' parameter is not set, using 'eval' split.\n",
      "INFO:absl:Evaluating model.\n",
      "INFO:absl:udf_utils.get_fn {'fairness_indicator_thresholds': 'null', 'example_splits': 'null', 'eval_config': '{\\n  \"metrics_specs\": [\\n    {\\n      \"per_slice_thresholds\": {\\n        \"sparse_categorical_accuracy\": {\\n          \"thresholds\": [\\n            {\\n              \"slicing_specs\": [\\n                {}\\n              ],\\n              \"threshold\": {\\n                \"change_threshold\": {\\n                  \"absolute\": -1e-10,\\n                  \"direction\": \"HIGHER_IS_BETTER\"\\n                },\\n                \"value_threshold\": {\\n                  \"lower_bound\": 0.6\\n                }\\n              }\\n            }\\n          ]\\n        }\\n      }\\n    }\\n  ],\\n  \"model_specs\": [\\n    {\\n      \"label_key\": \"species\"\\n    }\\n  ],\\n  \"slicing_specs\": [\\n    {},\\n    {\\n      \"feature_keys\": [\\n        \"species\"\\n      ]\\n    }\\n  ]\\n}'} 'custom_extractors'\n",
      "INFO:absl:Request was made to ignore the baseline ModelSpec and any change thresholds. This is likely because a baseline model was not provided: updated_config=\n",
      "model_specs {\n",
      "  label_key: \"species\"\n",
      "}\n",
      "slicing_specs {\n",
      "}\n",
      "slicing_specs {\n",
      "  feature_keys: \"species\"\n",
      "}\n",
      "metrics_specs {\n",
      "  model_names: \"\"\n",
      "  per_slice_thresholds {\n",
      "    key: \"sparse_categorical_accuracy\"\n",
      "    value {\n",
      "      thresholds {\n",
      "        slicing_specs {\n",
      "        }\n",
      "        threshold {\n",
      "          value_threshold {\n",
      "            lower_bound {\n",
      "              value: 0.6\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:Request was made to ignore the baseline ModelSpec and any change thresholds. This is likely because a baseline model was not provided: updated_config=\n",
      "model_specs {\n",
      "  label_key: \"species\"\n",
      "}\n",
      "slicing_specs {\n",
      "}\n",
      "slicing_specs {\n",
      "  feature_keys: \"species\"\n",
      "}\n",
      "metrics_specs {\n",
      "  model_names: \"\"\n",
      "  per_slice_thresholds {\n",
      "    key: \"sparse_categorical_accuracy\"\n",
      "    value {\n",
      "      thresholds {\n",
      "        slicing_specs {\n",
      "        }\n",
      "        threshold {\n",
      "          value_threshold {\n",
      "            lower_bound {\n",
      "              value: 0.6\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:Request was made to ignore the baseline ModelSpec and any change thresholds. This is likely because a baseline model was not provided: updated_config=\n",
      "model_specs {\n",
      "  label_key: \"species\"\n",
      "}\n",
      "slicing_specs {\n",
      "}\n",
      "slicing_specs {\n",
      "  feature_keys: \"species\"\n",
      "}\n",
      "metrics_specs {\n",
      "  model_names: \"\"\n",
      "  per_slice_thresholds {\n",
      "    key: \"sparse_categorical_accuracy\"\n",
      "    value {\n",
      "      thresholds {\n",
      "        slicing_specs {\n",
      "        }\n",
      "        threshold {\n",
      "          value_threshold {\n",
      "            lower_bound {\n",
      "              value: 0.6\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:Evaluation complete. Results written to pipelines/penguin-pipeline2/Evaluator/evaluation/46.\n",
      "INFO:absl:Checking validation results.\n",
      "INFO:absl:Blessing result True written to pipelines/penguin-pipeline2/Evaluator/blessing/46.\n",
      "INFO:absl:Cleaning up stateless execution info.\n",
      "INFO:absl:Execution 46 succeeded.\n",
      "INFO:absl:Cleaning up stateful execution info.\n",
      "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'evaluation': [Artifact(artifact: uri: \"pipelines/penguin-pipeline2/Evaluator/evaluation/46\"\n",
      ", artifact_type: name: \"ModelEvaluation\"\n",
      ")], 'blessing': [Artifact(artifact: uri: \"pipelines/penguin-pipeline2/Evaluator/blessing/46\"\n",
      ", artifact_type: name: \"ModelBlessing\"\n",
      ")]}) for execution 46\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Component Evaluator is finished.\n",
      "INFO:absl:Component Pusher is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.pusher.component.Pusher\"\n",
      "    base_type: DEPLOY\n",
      "  }\n",
      "  id: \"Pusher\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-pipeline2\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2023-04-27T11:58:59.502336\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-pipeline2.Pusher\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Trainer\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-pipeline2\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2023-04-27T11:58:59.502336\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-pipeline2.Trainer\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "            base_type: MODEL\n",
      "          }\n",
      "        }\n",
      "        output_key: \"model\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"model_blessing\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Evaluator\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-pipeline2\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2023-04-27T11:58:59.502336\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-pipeline2.Evaluator\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"ModelBlessing\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"blessing\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"pushed_model\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"PushedModel\"\n",
      "          base_type: MODEL\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"push_destination\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"filesystem\\\": {\\n    \\\"base_directory\\\": \\\"serving_model/penguin-pipeline2\\\"\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"Evaluator\"\n",
      "upstream_nodes: \"Trainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "INFO:absl:[Pusher] Resolved inputs: ({'model': [Artifact(artifact: id: 57\n",
      "type_id: 17\n",
      "uri: \"pipelines/penguin-pipeline2/Trainer/model/45\"\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.12.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1682596750775\n",
      "last_update_time_since_epoch: 1682596750775\n",
      ", artifact_type: id: 17\n",
      "name: \"Model\"\n",
      "base_type: MODEL\n",
      ")], 'model_blessing': [Artifact(artifact: id: 59\n",
      "type_id: 20\n",
      "uri: \"pipelines/penguin-pipeline2/Evaluator/blessing/46\"\n",
      "custom_properties {\n",
      "  key: \"blessed\"\n",
      "  value {\n",
      "    int_value: 1\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"current_model\"\n",
      "  value {\n",
      "    string_value: \"pipelines/penguin-pipeline2/Trainer/model/45\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"current_model_id\"\n",
      "  value {\n",
      "    int_value: 57\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.12.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1682596761138\n",
      "last_update_time_since_epoch: 1682596761138\n",
      ", artifact_type: id: 20\n",
      "name: \"ModelBlessing\"\n",
      ")]},)\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Going to run a new execution 47\n",
      "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=47, input_dict={'model': [Artifact(artifact: id: 57\n",
      "type_id: 17\n",
      "uri: \"pipelines/penguin-pipeline2/Trainer/model/45\"\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.12.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1682596750775\n",
      "last_update_time_since_epoch: 1682596750775\n",
      ", artifact_type: id: 17\n",
      "name: \"Model\"\n",
      "base_type: MODEL\n",
      ")], 'model_blessing': [Artifact(artifact: id: 59\n",
      "type_id: 20\n",
      "uri: \"pipelines/penguin-pipeline2/Evaluator/blessing/46\"\n",
      "custom_properties {\n",
      "  key: \"blessed\"\n",
      "  value {\n",
      "    int_value: 1\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"current_model\"\n",
      "  value {\n",
      "    string_value: \"pipelines/penguin-pipeline2/Trainer/model/45\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"current_model_id\"\n",
      "  value {\n",
      "    int_value: 57\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.12.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1682596761138\n",
      "last_update_time_since_epoch: 1682596761138\n",
      ", artifact_type: id: 20\n",
      "name: \"ModelBlessing\"\n",
      ")]}, output_dict=defaultdict(<class 'list'>, {'pushed_model': [Artifact(artifact: uri: \"pipelines/penguin-pipeline2/Pusher/pushed_model/47\"\n",
      ", artifact_type: name: \"PushedModel\"\n",
      "base_type: MODEL\n",
      ")]}), exec_properties={'custom_config': 'null', 'push_destination': '{\\n  \"filesystem\": {\\n    \"base_directory\": \"serving_model/penguin-pipeline2\"\\n  }\\n}'}, execution_output_uri='pipelines/penguin-pipeline2/Pusher/.system/executor_execution/47/executor_output.pb', stateful_working_dir='pipelines/penguin-pipeline2/Pusher/.system/stateful_working_dir/2023-04-27T11:58:59.502336', tmp_dir='pipelines/penguin-pipeline2/Pusher/.system/executor_execution/47/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.pusher.component.Pusher\"\n",
      "    base_type: DEPLOY\n",
      "  }\n",
      "  id: \"Pusher\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-pipeline2\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2023-04-27T11:58:59.502336\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-pipeline2.Pusher\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Trainer\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-pipeline2\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2023-04-27T11:58:59.502336\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-pipeline2.Trainer\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "            base_type: MODEL\n",
      "          }\n",
      "        }\n",
      "        output_key: \"model\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"model_blessing\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Evaluator\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-pipeline2\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2023-04-27T11:58:59.502336\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-pipeline2.Evaluator\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"ModelBlessing\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"blessing\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"pushed_model\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"PushedModel\"\n",
      "          base_type: MODEL\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"push_destination\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"filesystem\\\": {\\n    \\\"base_directory\\\": \\\"serving_model/penguin-pipeline2\\\"\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"Evaluator\"\n",
      "upstream_nodes: \"Trainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"penguin-pipeline2\"\n",
      ", pipeline_run_id='2023-04-27T11:58:59.502336')\n",
      "INFO:absl:Model version: 1682596761\n",
      "INFO:absl:Model written to serving path serving_model/penguin-pipeline2/1682596761.\n",
      "INFO:absl:Model pushed to pipelines/penguin-pipeline2/Pusher/pushed_model/47.\n",
      "INFO:absl:Cleaning up stateless execution info.\n",
      "INFO:absl:Execution 47 succeeded.\n",
      "INFO:absl:Cleaning up stateful execution info.\n",
      "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'pushed_model': [Artifact(artifact: uri: \"pipelines/penguin-pipeline2/Pusher/pushed_model/47\"\n",
      ", artifact_type: name: \"PushedModel\"\n",
      "base_type: MODEL\n",
      ")]}) for execution 47\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Component Pusher is finished.\n"
     ]
    }
   ],
   "source": [
    "#change to full data instead of incomplete\n",
    "tfx.orchestration.LocalDagRunner().run(\n",
    "_create_pipeline(\n",
    "    pipeline_name=PIPELINE_NAME,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    data_root=PROCESSED_DATA_ROOT,\n",
    "    #schema_path=SCHEMA_PATH,\n",
    "    module_file=_module_file,\n",
    "    serving_model_dir=SERVING_MODEL_DIR,\n",
    "    metadata_path=METADATA_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_metadata.proto import metadata_store_pb2\n",
    "# Non-public APIs, just for showcase.\n",
    "from tfx.orchestration.portable.mlmd import execution_lib\n",
    "\n",
    "# TODO(b/171447278): Move these functions into the TFX library.\n",
    "\n",
    "def get_latest_artifacts(metadata, pipeline_name, component_id):\n",
    "  \"\"\"Output artifacts of the latest run of the component.\"\"\"\n",
    "  context = metadata.store.get_context_by_type_and_name(\n",
    "      'node', f'{pipeline_name}.{component_id}')\n",
    "  #print(context)\n",
    "  executions = metadata.store.get_executions_by_context(context.id)\n",
    "  latest_execution = max(executions,\n",
    "                         key=lambda e:e.last_update_time_since_epoch)\n",
    "  return execution_lib.get_output_artifacts(metadata, latest_execution.id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:MetadataStore with DB connection initialized\n"
     ]
    }
   ],
   "source": [
    "# Non-public APIs, just for showcase.\n",
    "from tfx.orchestration.metadata import Metadata\n",
    "from tfx.types import standard_component_specs\n",
    "\n",
    "metadata_connection_config = tfx.orchestration.metadata.sqlite_metadata_connection_config(\n",
    "    METADATA_PATH)\n",
    "\n",
    "with Metadata(metadata_connection_config) as metadata_handler:\n",
    "  # Find output artifacts from MLMD.\n",
    "  evaluator_output = get_latest_artifacts(metadata_handler, PIPELINE_NAME,\n",
    "                                          'Evaluator')\n",
    "  eval_artifact = evaluator_output[standard_component_specs.EVALUATION_KEY][0]\n",
    "  #print(eval_artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvalResult(slicing_metrics=[((('species', 0),), {'': {'': {'sparse_categorical_accuracy': {'doubleValue': 0.978723406791687}, 'loss': {'doubleValue': 0.12361576408147812}}}}), ((), {'': {'': {'sparse_categorical_accuracy': {'doubleValue': 0.9599999785423279}, 'loss': {'doubleValue': 0.13475194573402405}}}}), ((('species', 1),), {'': {'': {'sparse_categorical_accuracy': {'doubleValue': 0.8636363744735718}, 'loss': {'doubleValue': 0.33218708634376526}}}}), ((('species', 2),), {'': {'': {'sparse_categorical_accuracy': {'doubleValue': 1.0}, 'loss': {'doubleValue': 0.011520599946379662}}}})], plots=[((('species', 0),), None), ((), None), ((('species', 1),), None), ((('species', 2),), None)], attributions=[((('species', 0),), None), ((), None), ((('species', 1),), None), ((('species', 2),), None)], config=model_specs {\n",
      "  label_key: \"species\"\n",
      "}\n",
      "slicing_specs {\n",
      "}\n",
      "slicing_specs {\n",
      "  feature_keys: \"species\"\n",
      "}\n",
      "metrics_specs {\n",
      "  model_names: \"\"\n",
      "  per_slice_thresholds {\n",
      "    key: \"sparse_categorical_accuracy\"\n",
      "    value {\n",
      "      thresholds {\n",
      "        slicing_specs {\n",
      "        }\n",
      "        threshold {\n",
      "          value_threshold {\n",
      "            lower_bound {\n",
      "              value: 0.6\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", data_location='<user provided PCollection>', file_format='<unknown>', model_location='pipelines/penguin-pipeline2/Trainer/model/40/Format-Serving')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba483dd1c2364c3da5dc08e27518c4e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SlicingMetricsViewer(config={'weightedExamplesColumn': 'example_count'}, data=[{'slice': 'species:0', 'metrics…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow_model_analysis as tfma\n",
    "#print(eval_artifact.uri)\n",
    "eval_result = tfma.load_eval_result(eval_artifact.uri)\n",
    "print(eval_result)\n",
    "tfma.view.render_slicing_metrics(eval_result, slicing_column='species')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serving_model/penguin-pipeline2\n",
      "serving_model/penguin-pipeline2/1682588571\n",
      "serving_model/penguin-pipeline2/1682588571/keras_metadata.pb\n",
      "serving_model/penguin-pipeline2/1682588571/saved_model.pb\n",
      "serving_model/penguin-pipeline2/1682588571/assets\n",
      "serving_model/penguin-pipeline2/1682588571/variables\n",
      "serving_model/penguin-pipeline2/1682588571/variables/variables.index\n",
      "serving_model/penguin-pipeline2/1682588571/variables/variables.data-00000-of-00001\n",
      "serving_model/penguin-pipeline2/1682588571/fingerprint.pb\n",
      "serving_model/penguin-pipeline2/1682589392\n",
      "serving_model/penguin-pipeline2/1682589392/keras_metadata.pb\n",
      "serving_model/penguin-pipeline2/1682589392/saved_model.pb\n",
      "serving_model/penguin-pipeline2/1682589392/assets\n",
      "serving_model/penguin-pipeline2/1682589392/variables\n",
      "serving_model/penguin-pipeline2/1682589392/variables/variables.index\n",
      "serving_model/penguin-pipeline2/1682589392/variables/variables.data-00000-of-00001\n",
      "serving_model/penguin-pipeline2/1682589392/fingerprint.pb\n",
      "serving_model/penguin-pipeline2/1682581514\n",
      "serving_model/penguin-pipeline2/1682581514/keras_metadata.pb\n",
      "serving_model/penguin-pipeline2/1682581514/saved_model.pb\n",
      "serving_model/penguin-pipeline2/1682581514/assets\n",
      "serving_model/penguin-pipeline2/1682581514/variables\n",
      "serving_model/penguin-pipeline2/1682581514/variables/variables.index\n",
      "serving_model/penguin-pipeline2/1682581514/variables/variables.data-00000-of-00001\n",
      "serving_model/penguin-pipeline2/1682581514/fingerprint.pb\n",
      "serving_model/penguin-pipeline2/1682511606\n",
      "serving_model/penguin-pipeline2/1682511606/keras_metadata.pb\n",
      "serving_model/penguin-pipeline2/1682511606/saved_model.pb\n",
      "serving_model/penguin-pipeline2/1682511606/assets\n",
      "serving_model/penguin-pipeline2/1682511606/variables\n",
      "serving_model/penguin-pipeline2/1682511606/variables/variables.index\n",
      "serving_model/penguin-pipeline2/1682511606/variables/variables.data-00000-of-00001\n",
      "serving_model/penguin-pipeline2/1682511606/fingerprint.pb\n",
      "serving_model/penguin-pipeline2/1682511688\n",
      "serving_model/penguin-pipeline2/1682511688/keras_metadata.pb\n",
      "serving_model/penguin-pipeline2/1682511688/saved_model.pb\n",
      "serving_model/penguin-pipeline2/1682511688/assets\n",
      "serving_model/penguin-pipeline2/1682511688/variables\n",
      "serving_model/penguin-pipeline2/1682511688/variables/variables.index\n",
      "serving_model/penguin-pipeline2/1682511688/variables/variables.data-00000-of-00001\n",
      "serving_model/penguin-pipeline2/1682511688/fingerprint.pb\n",
      "serving_model/penguin-pipeline2/1682510985\n",
      "serving_model/penguin-pipeline2/1682510985/keras_metadata.pb\n",
      "serving_model/penguin-pipeline2/1682510985/saved_model.pb\n",
      "serving_model/penguin-pipeline2/1682510985/assets\n",
      "serving_model/penguin-pipeline2/1682510985/variables\n",
      "serving_model/penguin-pipeline2/1682510985/variables/variables.index\n",
      "serving_model/penguin-pipeline2/1682510985/variables/variables.data-00000-of-00001\n",
      "serving_model/penguin-pipeline2/1682510985/fingerprint.pb\n"
     ]
    }
   ],
   "source": [
    "# List files in created model directory.\n",
    "!find {SERVING_MODEL_DIR}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-27 09:56:33.915685: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-27 09:56:34.205684: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-27 09:56:34.205734: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-04-27 09:56:35.406510: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-27 09:56:35.406631: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-27 09:56:35.406664: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "The given SavedModel SignatureDef contains the following input(s):\n",
      "  inputs['body_mass_g'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 1)\n",
      "      name: serving_default_body_mass_g:0\n",
      "  inputs['culmen_depth_mm'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 1)\n",
      "      name: serving_default_culmen_depth_mm:0\n",
      "  inputs['culmen_length_mm'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 1)\n",
      "      name: serving_default_culmen_length_mm:0\n",
      "  inputs['flipper_length_mm'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 1)\n",
      "      name: serving_default_flipper_length_mm:0\n",
      "The given SavedModel SignatureDef contains the following output(s):\n",
      "  outputs['dense_53'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 3)\n",
      "      name: StatefulPartitionedCall:0\n",
      "Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir {SERVING_MODEL_DIR}/$(ls -1 {SERVING_MODEL_DIR} | sort -nr | head -1) --tag_set serve --signature_def serving_default\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_dirs  <generator object <genexpr> at 0x7fece76ea5d0>\n",
      "max value  serving_model/penguin-pipeline2/1682589392  min value  serving_model/penguin-pipeline2/1682510985\n"
     ]
    }
   ],
   "source": [
    "# Find a model with the latest and oldest timestamp.\n",
    "model_dirs = (item for item in os.scandir(SERVING_MODEL_DIR) if item.is_dir())\n",
    "print('model_dirs ', (item for item in os.scandir(SERVING_MODEL_DIR) if item.is_dir()))\n",
    "model_path_new = max(model_dirs, key=lambda i: int(i.name)).path\n",
    "\n",
    "model_dirs = (item for item in os.scandir(SERVING_MODEL_DIR) if item.is_dir())\n",
    "model_path_old = min(model_dirs, key=lambda i: int(i.name)).path\n",
    "print('max value ', model_path_new, ' min value ', model_path_old)\n",
    "loaded_model_new = tf.keras.models.load_model(model_path_new)\n",
    "loaded_model_old = tf.keras.models.load_model(model_path_old)\n",
    "inference_fn_new = loaded_model_new.signatures['serving_default']\n",
    "inference_fn_old = loaded_model_old.signatures['serving_default']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "signature_wrapper(*, culmen_depth_mm, body_mass_g, culmen_length_mm, flipper_length_mm) missing required arguments: body_mass_g, culmen_depth_mm, culmen_length_mm, flipper_length_mm.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_with_flat_signature\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1517\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'body_mass_g'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1487\u001b[0m             return self._call_with_flat_signature(args, kwargs,\n\u001b[0;32m-> 1488\u001b[0;31m                                                   cancellation_manager)\n\u001b[0m\u001b[1;32m   1489\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_with_flat_signature\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1523\u001b[0m             set(self._arg_keywords) - set(specified_keywords))\n\u001b[0;32m-> 1524\u001b[0;31m         raise TypeError(f\"{self._flat_signature_summary()} missing required \"\n\u001b[0m\u001b[1;32m   1525\u001b[0m                         f\"arguments: {', '.join(missing_required_args)}.\")\n",
      "\u001b[0;31mTypeError\u001b[0m: signature_wrapper(body_mass_g, culmen_depth_mm, culmen_length_mm, flipper_length_mm) missing required arguments: body_mass_g, culmen_depth_mm, culmen_length_mm, flipper_length_mm.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2037/3056915169.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexample_proto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mresult_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minference_fn_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Chinstrap test: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model with incomplete dataset result: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'output_0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1472\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mdo\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1473\u001b[0m     \"\"\"\n\u001b[0;32m-> 1474\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1476\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1488\u001b[0m                                                   cancellation_manager)\n\u001b[1;32m   1489\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1490\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mstructured_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1492\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_with_flat_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m           return self._call_with_structured_signature(args, kwargs,\n\u001b[0;32m-> 1484\u001b[0;31m                                                       cancellation_manager)\n\u001b[0m\u001b[1;32m   1485\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstructured_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m           \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_with_structured_signature\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1559\u001b[0m     args, kwargs, filtered_flat_args = (\n\u001b[1;32m   1560\u001b[0m         self._function_spec.canonicalize_function_inputs(args, kwargs))\n\u001b[0;32m-> 1561\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structured_signature_check_missing_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1562\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structured_signature_check_unexpected_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1563\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structured_signature_check_arg_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_structured_signature_check_missing_args\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m         \u001b[0mmissing_arguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmissing_arguments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1581\u001b[0;31m       raise TypeError(f\"{self._structured_signature_summary()} missing \"\n\u001b[0m\u001b[1;32m   1582\u001b[0m                       \u001b[0;34m\"required arguments: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1583\u001b[0m                       f\"{', '.join(sorted(missing_arguments))}.\")\n",
      "\u001b[0;31mTypeError\u001b[0m: signature_wrapper(*, culmen_depth_mm, body_mass_g, culmen_length_mm, flipper_length_mm) missing required arguments: body_mass_g, culmen_depth_mm, culmen_length_mm, flipper_length_mm."
     ]
    }
   ],
   "source": [
    "# Prepare an example and run inference.\n",
    "\n",
    "#Chinstrap test\n",
    "features = {\n",
    "  'culmen_length_mm': tf.train.Feature(float_list=tf.train.FloatList(value=[46.3])),\n",
    "  'culmen_depth_mm': tf.train.Feature(float_list=tf.train.FloatList(value=[17.5])),\n",
    "  'flipper_length_mm': tf.train.Feature(int64_list=tf.train.Int64List(value=[187])),\n",
    "  'body_mass_g': tf.train.Feature(int64_list=tf.train.Int64List(value=[3200])),\n",
    "}\n",
    "example_proto = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "examples = example_proto.SerializeToString()\n",
    "\n",
    "result_new = inference_fn_new(examples=tf.constant([examples]))\n",
    "print('Chinstrap test: ')\n",
    "print('Model with incomplete dataset result: ', result_new['output_0'].numpy())\n",
    "\n",
    "result_old = inference_fn_old(examples=tf.constant([examples]))\n",
    "print('Model with full dataset result: ', result_old['output_0'].numpy())\n",
    " \n",
    "#Adelie test\n",
    "features = {\n",
    "  'culmen_length_mm': tf.train.Feature(float_list=tf.train.FloatList(value=[34.9])),\n",
    "  'culmen_depth_mm': tf.train.Feature(float_list=tf.train.FloatList(value=[17.5])),\n",
    "  'flipper_length_mm': tf.train.Feature(int64_list=tf.train.Int64List(value=[190])),\n",
    "  'body_mass_g': tf.train.Feature(int64_list=tf.train.Int64List(value=[3723])),\n",
    "}\n",
    "example_proto = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "examples = example_proto.SerializeToString()\n",
    "print('Adelie test: ')\n",
    "result_new = inference_fn_new(examples=tf.constant([examples]))\n",
    "print('Model with incomplete dataset result: ', result_new['output_0'].numpy())\n",
    "\n",
    "result_old = inference_fn_old(examples=tf.constant([examples]))\n",
    "print('Model with full dataset result: ', result_old['output_0'].numpy())\n",
    "\n",
    "#Gentoo test \n",
    "features = {\n",
    "  'culmen_length_mm': tf.train.Feature(float_list=tf.train.FloatList(value=[49.5])),\n",
    "  'culmen_depth_mm': tf.train.Feature(float_list=tf.train.FloatList(value=[16.5])),\n",
    "  'flipper_length_mm': tf.train.Feature(int64_list=tf.train.Int64List(value=[227])),\n",
    "  'body_mass_g': tf.train.Feature(int64_list=tf.train.Int64List(value=[6100])),\n",
    "}\n",
    "example_proto = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "examples = example_proto.SerializeToString()\n",
    "print('Gentoo test: ')\n",
    "result_new = inference_fn_new(examples=tf.constant([examples]))\n",
    "print('Model with incomplete dataset result: ', result_new['output_0'].numpy())\n",
    "\n",
    "result_old = inference_fn_old(examples=tf.constant([examples]))\n",
    "print('Model with full dataset result: ', result_old['output_0'].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head{_data_filepath}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLMD DATABASE QUERY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import urllib\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow_model_analysis as tfma\n",
    "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_config = tfx.orchestration.metadata.sqlite_metadata_connection_config(METADATA_PATH)\n",
    "store = mlmd.MetadataStore(connection_config)\n",
    "\n",
    "# All TFX artifacts are stored in the base directory\n",
    "base_dir = connection_config.sqlite.filename_uri.split('metadata.sqlite')[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_types(types):\n",
    "  # Helper function to render dataframes for the artifact and execution types\n",
    "  table = {'id': [], 'name': []}\n",
    "  for a_type in types:\n",
    "    table['id'].append(a_type.id)\n",
    "    table['name'].append(a_type.name)\n",
    "  return pd.DataFrame(data=table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_artifacts(store, artifacts):\n",
    "  # Helper function to render dataframes for the input artifacts\n",
    "  table = {'artifact id': [], 'type': [], 'uri': []}\n",
    "  for a in artifacts:\n",
    "    table['artifact id'].append(a.id)\n",
    "    artifact_type = store.get_artifact_types_by_id([a.type_id])[0]\n",
    "    table['type'].append(artifact_type.name)\n",
    "    table['uri'].append(a.uri.replace(base_dir, './'))\n",
    "  return pd.DataFrame(data=table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_properties(store, node):\n",
    "  # Helper function to render dataframes for artifact and execution properties\n",
    "  table = {'property': [], 'value': []}\n",
    "  for k, v in node.properties.items():\n",
    "    table['property'].append(k)\n",
    "    table['value'].append(\n",
    "        v.string_value if v.HasField('string_value') else v.int_value)\n",
    "  for k, v in node.custom_properties.items():\n",
    "    table['property'].append(k)\n",
    "    table['value'].append(\n",
    "        v.string_value if v.HasField('string_value') else v.int_value)\n",
    "  return pd.DataFrame(data=table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_types(store.get_artifact_types())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pushed_models = store.get_artifacts_by_type(\"PushedModel\")\n",
    "display_artifacts(store, pushed_models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pushed_model = pushed_models[-1]\n",
    "display_properties(store, pushed_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hop_parent_artifacts(store, artifacts):\n",
    "  # Get a list of artifacts within a 1-hop of the artifacts of interest\n",
    "  artifact_ids = [artifact.id for artifact in artifacts]\n",
    "  executions_ids = set(\n",
    "      event.execution_id\n",
    "      for event in store.get_events_by_artifact_ids(artifact_ids)\n",
    "      if event.type == mlmd.proto.Event.OUTPUT)\n",
    "  artifacts_ids = set(\n",
    "      event.artifact_id\n",
    "      for event in store.get_events_by_execution_ids(executions_ids)\n",
    "      if event.type == mlmd.proto.Event.INPUT)\n",
    "  return [artifact for artifact in store.get_artifacts_by_id(artifacts_ids)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_artifacts = get_one_hop_parent_artifacts(store, [pushed_model])\n",
    "display_artifacts(store, parent_artifacts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exported_model = parent_artifacts[0]\n",
    "display_properties(store, exported_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parents = get_one_hop_parent_artifacts(store, [exported_model])\n",
    "display_artifacts(store, model_parents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_data = model_parents[0]\n",
    "display_properties(store, used_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_types(store.get_execution_types())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_producer_execution(store, artifact):\n",
    "  executions_ids = set(\n",
    "      event.execution_id\n",
    "      for event in store.get_events_by_artifact_ids([artifact.id])\n",
    "      if event.type == mlmd.proto.Event.OUTPUT)\n",
    "  return store.get_executions_by_id(executions_ids)[0]\n",
    "\n",
    "trainer = find_producer_execution(store, exported_model)\n",
    "display_properties(store, trainer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
